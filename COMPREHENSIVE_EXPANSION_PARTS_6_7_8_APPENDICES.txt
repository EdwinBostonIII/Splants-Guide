=== CONTINUATION OF PART 6 ===

SECTION 6.2: ALERT CONFIGURATION

Purpose: Receive timely notifications about problems without being overwhelmed
by false positives or irrelevant alerts.

6.2.1 Alert Hierarchy and Response Requirements

Alert classification matrix:

┌──────────┬─────────────────────────┬──────────────────────┬────────────────┐
│ Level    │ Examples                │ Response Time        │ Notification   │
├──────────┼─────────────────────────┼──────────────────────┼────────────────┤
│ CRITICAL │ • Payment processing    │ Immediate            │ • PagerDuty    │
│ (SEV-1)  │   completely down       │ ACK: 5 minutes       │ • Phone call   │
│          │ • Database unreachable  │ FIX: 30-60 minutes   │ • Discord @here│
│          │ • All providers failing │                      │ • SMS          │
│          │ • Security breach       │                      │                │
├──────────┼─────────────────────────┼──────────────────────┼────────────────┤
│ HIGH     │ • Single provider down  │ Urgent               │ • Discord      │
│ (SEV-2)  │ • Error rate > 10%      │ ACK: 15 minutes      │   @channel     │
│          │ • Manual queue overflow │ FIX: 2-4 hours       │ • Email        │
│          │ • Webhook delay > 10min │                      │ • Slack        │
├──────────┼─────────────────────────┼──────────────────────┼────────────────┤
│ WARNING  │ • Performance degraded  │ Scheduled            │ • Discord msg  │
│ (SEV-3)  │ • Cost anomaly detected │ ACK: 2 hours         │ • Email        │
│          │ • Error rate 2-10%      │ FIX: 24 hours        │                │
│          │ • Storage 80% full      │                      │                │
├──────────┼─────────────────────────┼──────────────────────┼────────────────┤
│ INFO     │ • Daily summary         │ No action required   │ • Email digest │
│          │ • Successful deployment │                      │ • Log only     │
│          │ • Milestone reached     │                      │                │
└──────────┴─────────────────────────┴──────────────────────┴────────────────┘

6.2.2 Alert Rule Implementation Examples

Payment processing stopped:
```sql
-- Create alert check function
CREATE OR REPLACE FUNCTION check_payment_processing() RETURNS TABLE(
  is_alert BOOLEAN,
  severity TEXT,
  message TEXT,
  affected_orders INTEGER
) AS $$
DECLARE
  recent_orders INTEGER;
  current_hour INTEGER;
  is_business_hours BOOLEAN;
BEGIN
  -- Get current hour (0-23)
  current_hour := EXTRACT(HOUR FROM NOW() AT TIME ZONE 'America/New_York');
  
  -- Business hours: 6am-11pm EST
  is_business_hours := current_hour >= 6 AND current_hour < 23;
  
  -- Count orders in last 10 minutes
  SELECT COUNT(*) INTO recent_orders
  FROM orders
  WHERE created_at > NOW() - INTERVAL '10 minutes';
  
  -- Alert if no orders during business hours
  IF recent_orders = 0 AND is_business_hours THEN
    RETURN QUERY SELECT
      true AS is_alert,
      'CRITICAL' AS severity,
      'No orders processed in last 10 minutes during business hours' AS message,
      0 AS affected_orders;
  ELSE
    RETURN QUERY SELECT
      false AS is_alert,
      'INFO' AS severity,
      format('%s orders in last 10 minutes', recent_orders) AS message,
      recent_orders AS affected_orders;
  END IF;
END;
$$ LANGUAGE plpgsql;

-- Run this every 5 minutes via Make.com or pg_cron
```

Error rate threshold:
```sql
-- Check if error rate exceeds acceptable levels
CREATE OR REPLACE FUNCTION check_error_rate() RETURNS TABLE(
  is_alert BOOLEAN,
  severity TEXT,
  error_rate_percent NUMERIC,
  error_count INTEGER,
  total_events INTEGER
) AS $$
DECLARE
  errors INTEGER;
  total INTEGER;
  rate NUMERIC;
BEGIN
  SELECT
    COUNT(*) FILTER (WHERE level IN ('error', 'critical')),
    COUNT(*)
  INTO errors, total
  FROM system_logs
  WHERE timestamp > NOW() - INTERVAL '5 minutes';
  
  rate := ROUND((errors::NUMERIC / NULLIF(total, 0)) * 100, 2);
  
  IF rate >= 10 THEN
    RETURN QUERY SELECT
      true,
      'HIGH',
      rate,
      errors,
      total;
  ELSIF rate >= 5 THEN
    RETURN QUERY SELECT
      true,
      'WARNING',
      rate,
      errors,
      total;
  ELSE
    RETURN QUERY SELECT
      false,
      'INFO',
      rate,
      errors,
      total;
  END IF;
END;
$$ LANGUAGE plpgsql;
```

Provider performance degradation:
```sql
-- Alert if provider response times significantly exceed baseline
CREATE OR REPLACE FUNCTION check_provider_performance() RETURNS TABLE(
  provider_name TEXT,
  is_alert BOOLEAN,
  severity TEXT,
  current_p95_ms INTEGER,
  baseline_p95_ms INTEGER,
  degradation_percent NUMERIC
) AS $$
BEGIN
  RETURN QUERY
  WITH current_performance AS (
    SELECT
      f.provider_name,
      percentile_cont(0.95) WITHIN GROUP (ORDER BY response_time_ms) AS p95_ms
    FROM fulfillment_events f
    WHERE created_at > NOW() - INTERVAL '10 minutes'
      AND response_time_ms IS NOT NULL
    GROUP BY f.provider_name
  ),
  baseline_performance AS (
    SELECT
      f.provider_name,
      percentile_cont(0.95) WITHIN GROUP (ORDER BY response_time_ms) AS baseline_p95
    FROM fulfillment_events f
    WHERE created_at > NOW() - INTERVAL '7 days'
      AND created_at < NOW() - INTERVAL '1 hour'
      AND response_time_ms IS NOT NULL
    GROUP BY f.provider_name
  )
  SELECT
    cp.provider_name,
    (cp.p95_ms > bp.baseline_p95 * 2) AS is_alert,
    CASE
      WHEN cp.p95_ms > bp.baseline_p95 * 3 THEN 'HIGH'
      WHEN cp.p95_ms > bp.baseline_p95 * 2 THEN 'WARNING'
      ELSE 'INFO'
    END AS severity,
    cp.p95_ms::INTEGER,
    bp.baseline_p95::INTEGER,
    ROUND(((cp.p95_ms / NULLIF(bp.baseline_p95, 0)) - 1) * 100, 1) AS degradation_percent
  FROM current_performance cp
  JOIN baseline_performance bp ON cp.provider_name = bp.provider_name
  WHERE cp.p95_ms > bp.baseline_p95 * 1.5;
END;
$$ LANGUAGE plpgsql;
```

Manual queue capacity:
```sql
-- Alert if manual queue exceeds capacity or has urgent items aging
CREATE OR REPLACE FUNCTION check_manual_queue() RETURNS TABLE(
  is_alert BOOLEAN,
  severity TEXT,
  message TEXT,
  queue_depth INTEGER,
  urgent_count INTEGER,
  oldest_urgent_age_minutes INTEGER
) AS $$
DECLARE
  pending_count INTEGER;
  urgent_pending INTEGER;
  oldest_age INTEGER;
BEGIN
  SELECT
    COUNT(*),
    COUNT(*) FILTER (WHERE priority IN ('high', 'urgent')),
    COALESCE(MAX(EXTRACT(EPOCH FROM (NOW() - created_at)) / 60)::INTEGER, 0) FILTER (WHERE priority = 'urgent')
  INTO pending_count, urgent_pending, oldest_age
  FROM manual_queue
  WHERE status = 'pending';
  
  IF urgent_pending > 0 AND oldest_age > 120 THEN
    RETURN QUERY SELECT
      true,
      'CRITICAL',
      format('%s urgent items in queue, oldest is %s minutes old', urgent_pending, oldest_age),
      pending_count,
      urgent_pending,
      oldest_age;
  ELSIF pending_count > 50 THEN
    RETURN QUERY SELECT
      true,
      'HIGH',
      format('Manual queue at %s items (capacity threshold)', pending_count),
      pending_count,
      urgent_pending,
      oldest_age;
  ELSIF urgent_pending > 0 AND oldest_age > 60 THEN
    RETURN QUERY SELECT
      true,
      'WARNING',
      format('%s urgent items, oldest is %s minutes old', urgent_pending, oldest_age),
      pending_count,
      urgent_pending,
      oldest_age;
  ELSE
    RETURN QUERY SELECT
      false,
      'INFO',
      format('%s items in queue', pending_count),
      pending_count,
      urgent_pending,
      oldest_age;
  END IF;
END;
$$ LANGUAGE plpgsql;
```

Cost anomaly detection:
```sql
-- Alert if daily costs significantly exceed baseline
CREATE OR REPLACE FUNCTION check_cost_anomaly() RETURNS TABLE(
  is_alert BOOLEAN,
  severity TEXT,
  today_cost_dollars NUMERIC,
  baseline_cost_dollars NUMERIC,
  variance_percent NUMERIC
) AS $$
DECLARE
  today_cost INTEGER;
  baseline_avg INTEGER;
  variance NUMERIC;
BEGIN
  -- Today's costs so far
  SELECT COALESCE(SUM(cost_cents + shipping_cost_cents), 0)
  INTO today_cost
  FROM fulfillment_events
  WHERE DATE(created_at) = CURRENT_DATE
    AND status = 'submitted';
  
  -- Average daily cost over last 30 days
  SELECT COALESCE(AVG(daily_cost), 0)::INTEGER
  INTO baseline_avg
  FROM (
    SELECT SUM(cost_cents + shipping_cost_cents) AS daily_cost
    FROM fulfillment_events
    WHERE created_at >= CURRENT_DATE - INTERVAL '30 days'
      AND created_at < CURRENT_DATE
      AND status = 'submitted'
    GROUP BY DATE(created_at)
  ) AS daily_costs;
  
  variance := ROUND(((today_cost::NUMERIC / NULLIF(baseline_avg, 0)) - 1) * 100, 1);
  
  IF variance > 50 THEN
    RETURN QUERY SELECT
      true,
      'HIGH',
      ROUND(today_cost / 100.0, 2),
      ROUND(baseline_avg / 100.0, 2),
      variance;
  ELSIF variance > 25 THEN
    RETURN QUERY SELECT
      true,
      'WARNING',
      ROUND(today_cost / 100.0, 2),
      ROUND(baseline_avg / 100.0, 2),
      variance;
  ELSE
    RETURN QUERY SELECT
      false,
      'INFO',
      ROUND(today_cost / 100.0, 2),
      ROUND(baseline_avg / 100.0, 2),
      variance;
  END IF;
END;
$$ LANGUAGE plpgsql;
```

6.2.3 Alert Delivery and Escalation

Discord webhook integration:
```javascript
// Send alert to Discord
async function sendDiscordAlert(severity, title, message, context = {}) {
  const colors = {
    'CRITICAL': 15158332, // Red
    'HIGH': 15105570,     // Orange
    'WARNING': 16776960,  // Yellow
    'INFO': 3447003       // Blue
  };
  
  const mentions = {
    'CRITICAL': '@here ',
    'HIGH': '@channel ',
    'WARNING': '',
    'INFO': ''
  };
  
  const embed = {
    title: `${severity}: ${title}`,
    description: message,
    color: colors[severity],
    fields: Object.entries(context).map(([key, value]) => ({
      name: key.replace(/_/g, ' ').replace(/\b\w/g, l => l.toUpperCase()),
      value: String(value),
      inline: true
    })),
    timestamp: new Date().toISOString(),
    footer: {
      text: 'Splants Automation Monitoring'
    }
  };
  
  await fetch(process.env.DISCORD_WEBHOOK_URL, {
    method: 'POST',
    headers: { 'Content-Type': 'application/json' },
    body: JSON.stringify({
      content: mentions[severity] + `**${severity} Alert**`,
      embeds: [embed]
    })
  });
}

// Usage
await sendDiscordAlert(
  'HIGH',
  'Printful API Performance Degraded',
  'P95 response time increased from 850ms baseline to 2400ms (182% increase)',
  {
    provider: 'Printful',
    current_p95: '2400ms',
    baseline_p95: '850ms',
    affected_orders: 15,
    recommendation: 'Consider temporary failover to Printify'
  }
);
```

PagerDuty integration for critical alerts:
```javascript
// Trigger PagerDuty incident
async function triggerPagerDutyIncident(title, details, severity = 'critical') {
  const response = await fetch('https://api.pagerduty.com/incidents', {
    method: 'POST',
    headers: {
      'Authorization': `Token token=${process.env.PAGERDUTY_API_KEY}`,
      'Content-Type': 'application/json',
      'Accept': 'application/vnd.pagerduty+json;version=2',
      'From': 'alerts@yourstore.com'
    },
    body: JSON.stringify({
      incident: {
        type: 'incident',
        title: title,
        service: {
          id: process.env.PAGERDUTY_SERVICE_ID,
          type: 'service_reference'
        },
        urgency: severity === 'critical' ? 'high' : 'low',
        body: {
          type: 'incident_body',
          details: JSON.stringify(details, null, 2)
        }
      }
    })
  });
  
  const incident = await response.json();
  return incident.incident.id;
}

// Usage for critical payment processing failure
const incidentId = await triggerPagerDutyIncident(
  'Payment Processing Down: No orders in 10 minutes',
  {
    alert_time: new Date().toISOString(),
    last_successful_order: '2025-11-16T10:45:23Z',
    minutes_since_last_order: 12,
    business_hours: true,
    stripe_status: 'Operational',
    makecom_status: 'Checking...',
    database_status: 'Reachable',
    recommended_actions: [
      'Check Make.com scenario status',
      'Verify webhook endpoint responding',
      'Check Stripe webhook delivery logs',
      'Review recent deployment changes'
    ]
  },
  'critical'
);
```

Escalation policy implementation:
```javascript
// Escalation state machine
const escalationPolicy = {
  'CRITICAL': [
    { delay: 0, action: 'pagerduty', target: 'on-call' },
    { delay: 300, action: 'phone', target: 'backup-engineer' },
    { delay: 900, action: 'phone', target: 'engineering-lead' },
    { delay: 1800, action: 'phone', target: 'founder' }
  ],
  'HIGH': [
    { delay: 0, action: 'discord', target: 'eng-channel' },
    { delay: 900, action: 'email', target: 'on-call' },
    { delay: 3600, action: 'discord', target: 'lead-mention' }
  ],
  'WARNING': [
    { delay: 0, action: 'discord', target: 'ops-channel' },
    { delay: 7200, action: 'email', target: 'team-list' }
  ]
};

async function executeEscalation(alertId, severity, title, details) {
  const policy = escalationPolicy[severity];
  const alert = await getAlertState(alertId);
  
  for (const step of policy) {
    // Check if alert has been acknowledged
    if (alert.acknowledged_at) {
      console.log(`Alert ${alertId} acknowledged, stopping escalation`);
      break;
    }
    
    // Wait for delay
    if (step.delay > 0) {
      await new Promise(resolve => setTimeout(resolve, step.delay * 1000));
      
      // Recheck acknowledgment after delay
      const updated = await getAlertState(alertId);
      if (updated.acknowledged_at) {
        break;
      }
    }
    
    // Execute escalation action
    switch (step.action) {
      case 'pagerduty':
        await triggerPagerDutyIncident(title, details, severity.toLowerCase());
        break;
      case 'discord':
        await sendDiscordAlert(severity, title, JSON.stringify(details, null, 2));
        break;
      case 'email':
        await sendEmailAlert(step.target, severity, title, details);
        break;
      case 'phone':
        await initiatePhoneCall(step.target, title);
        break;
    }
    
    await logEscalationStep(alertId, step);
  }
}
```

6.2.4 Alert Fatigue Prevention

Implement these patterns to maintain signal-to-noise ratio:

Alert grouping:
```javascript
// Group related alerts within time window
const alertBuffer = new Map();
const GROUP_WINDOW_MS = 120000; // 2 minutes

async function processAlert(alert) {
  const groupKey = `${alert.service}_${alert.check_name}`;
  
  if (alertBuffer.has(groupKey)) {
    const existingGroup = alertBuffer.get(groupKey);
    existingGroup.occurrences.push(alert);
    existingGroup.latest_occurrence = new Date();
  } else {
    alertBuffer.set(groupKey, {
      first_occurrence: new Date(),
      latest_occurrence: new Date(),
      occurrences: [alert],
      groupKey: groupKey
    });
    
    // Schedule group flush after window
    setTimeout(() => flushAlertGroup(groupKey), GROUP_WINDOW_MS);
  }
}

async function flushAlertGroup(groupKey) {
  const group = alertBuffer.get(groupKey);
  if (!group) return;
  
  alertBuffer.delete(groupKey);
  
  if (group.occurrences.length === 1) {
    // Single occurrence, send as-is
    await sendAlert(group.occurrences[0]);
  } else {
    // Multiple occurrences, send grouped summary
    await sendAlert({
      severity: group.occurrences[0].severity,
      title: `${group.occurrences[0].title} (${group.occurrences.length} occurrences)`,
      message: `This alert fired ${group.occurrences.length} times in ${Math.round((group.latest_occurrence - group.first_occurrence) / 1000)} seconds`,
      details: {
        first_occurrence: group.first_occurrence,
        latest_occurrence: group.latest_occurrence,
        occurrence_count: group.occurrences.length,
        sample_details: group.occurrences.slice(0, 3).map(a => a.details)
      }
    });
  }
}
```

Hysteresis to prevent flapping:
```javascript
// Alert state tracker with hysteresis
class AlertStateManager {
  constructor() {
    this.states = new Map();
  }
  
  shouldAlert(alertName, currentValue, thresholds) {
    const state = this.states.get(alertName) || { alerting: false };
    
    // Different thresholds for entering and exiting alert state
    const enterThreshold = thresholds.enter;
    const exitThreshold = thresholds.exit;
    
    if (!state.alerting && currentValue >= enterThreshold) {
      // Cross into alert territory
      this.states.set(alertName, { alerting: true, since: new Date() });
      return { shouldAlert: true, reason: 'threshold_exceeded', value: currentValue };
    }
    
    if (state.alerting && currentValue <= exitThreshold) {
      // Recovered below exit threshold
      const duration = new Date() - state.since;
      this.states.set(alertName, { alerting: false });
      return { shouldAlert: false, reason: 'threshold_recovered', duration_ms: duration };
    }
    
    return { shouldAlert: false, reason: 'no_state_change', alerting: state.alerting };
  }
}

// Usage
const alertManager = new AlertStateManager();

// Error rate with hysteresis: alert at 10%, clear at 5%
const result = alertManager.shouldAlert('error_rate', currentErrorRate, {
  enter: 10.0,
  exit: 5.0
});

if (result.shouldAlert) {
  await sendAlert({
    severity: 'HIGH',
    title: 'Error Rate Threshold Exceeded',
    message: `Error rate at ${currentErrorRate}%, threshold is ${10}%`,
    details: result
  });
}
```

Maintenance window silencing:
```sql
-- Maintenance windows table
CREATE TABLE maintenance_windows (
  id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
  title TEXT NOT NULL,
  description TEXT,
  start_time TIMESTAMP NOT NULL,
  end_time TIMESTAMP NOT NULL,
  affected_services TEXT[] NOT NULL,
  created_by TEXT NOT NULL,
  created_at TIMESTAMP NOT NULL DEFAULT NOW()
);

CREATE INDEX idx_maintenance_active ON maintenance_windows(start_time, end_time)
  WHERE end_time > NOW();

-- Check if alert should be silenced
CREATE OR REPLACE FUNCTION is_silenced_by_maintenance(
  service_name TEXT,
  check_time TIMESTAMP DEFAULT NOW()
) RETURNS BOOLEAN AS $$
BEGIN
  RETURN EXISTS (
    SELECT 1 FROM maintenance_windows
    WHERE check_time BETWEEN start_time AND end_time
      AND service_name = ANY(affected_services)
  );
END;
$$ LANGUAGE plpgsql;
```

Alert suppression during known issues:
```javascript
// Suppress alerts for known issues
const knownIssues = new Map();

function registerKnownIssue(issueId, pattern, suppressionDuration = 3600000) {
  knownIssues.set(issueId, {
    pattern: pattern,
    registered_at: new Date(),
    expires_at: new Date(Date.now() + suppressionDuration),
    suppressed_count: 0
  });
}

function shouldSuppressAlert(alert) {
  for (const [issueId, known] of knownIssues.entries()) {
    if (new Date() > known.expires_at) {
      knownIssues.delete(issueId);
      continue;
    }
    
    if (matchesPattern(alert, known.pattern)) {
      known.suppressed_count++;
      console.log(`Alert suppressed due to known issue ${issueId} (${known.suppressed_count} suppressed so far)`);
      return true;
    }
  }
  return false;
}

// Usage during incident
registerKnownIssue(
  'PRINT-2025-11-16-001',
  { service: 'printful_api', error_contains: 'timeout' },
  3600000 // Suppress for 1 hour
);
```

Production Reality Box:
┌─────────────────────────────────────────────────────────────────────────────┐
│ PRODUCTION REALITY: Alert Fatigue Killed Response Culture                   │
│                                                                             │
│ One team configured 52 different alerts with no grouping or hysteresis.     │
│ Within 2 weeks, engineers received 300-400 notifications daily. The team    │
│ started ignoring all alerts. A real SEV-1 database outage went unnoticed    │
│ for 47 minutes because everyone assumed it was noise. After reducing to 12  │
│ high-quality alerts with proper grouping, false positive rate dropped from  │
│ 73% to 4%, and MTTA (mean time to acknowledgment) improved from 22 minutes │
│ to 3 minutes. Quality over quantity is critical for effective alerting.     │
└─────────────────────────────────────────────────────────────────────────────┘

Validation checkpoint:
  □ Alert hierarchy defined with clear severity levels and response SLAs
  □ All critical failure modes covered by automated checks
  □ Alerts route to appropriate channels based on severity
  □ Escalation policies tested end-to-end
  □ Alert grouping prevents notification storms
  □ Hysteresis prevents flapping between states
  □ Maintenance windows and known issue suppression working
  □ False positive rate below 5 percent after tuning period

═══════════════════════════════════════════════════════════════════════════════



SECTION 6.3: INCIDENT RESPONSE PROCEDURES

Purpose: Restore service rapidly and systematically when failures occur,
minimizing customer impact and revenue loss.

6.3.1 Incident Response Framework

Incident lifecycle:
```
┌─────────────┐     ┌─────────────┐     ┌─────────────┐     ┌─────────────┐
│  DETECTION  │────▶│ TRIAGE AND  │────▶│  MITIGATION │────▶│ RESOLUTION  │
│             │     │ ESCALATION  │     │             │     │             │
│ • Automated │     │ • Classify  │     │ • Stop      │     │ • Root      │
│   checks    │     │   severity  │     │   bleeding  │     │   cause     │
│ • User      │     │ • Notify    │     │ • Restore   │     │ • Deploy    │
│   reports   │     │   team      │     │   service   │     │   fix       │
│ • Proactive │     │ • Start     │     │ • Comm out  │     │ • Document  │
│   discovery │     │   timer     │     │             │     │ • Postmort  │
└─────────────┘     └─────────────┘     └─────────────┘     └─────────────┘
```

6.3.2 Payment Processing Failure Runbook

Symptom: No orders received in 10+ minutes during business hours, or Stripe
webhooks not processing.

Step 1: Verify the problem (60 seconds)
```sql
-- Check recent order flow
SELECT
  COUNT(*) AS orders_last_15_min,
  MAX(created_at) AS most_recent_order,
  EXTRACT(EPOCH FROM (NOW() - MAX(created_at))) / 60 AS minutes_since_last
FROM orders
WHERE created_at > NOW() - INTERVAL '15 minutes';

-- Check Stripe webhook deliveries
SELECT
  webhook_id,
  event_type,
  created_at,
  status,
  error_message
FROM stripe_webhooks
WHERE created_at > NOW() - INTERVAL '30 minutes'
ORDER BY created_at DESC
LIMIT 20;

-- Check Make.com scenario status via API
curl -X GET "https://api.make.com/v2/scenarios/${SCENARIO_ID}/executions" \
  -H "Authorization: Token ${MAKE_API_KEY}" \
  | jq '.data[0:5] | .[] | {time: .createdDate, status: .status, error: .error}'
```

Step 2: Check external service status (30 seconds)
- Stripe status: https://status.stripe.com
- Make.com status: https://status.make.com
- Your hosting status: Check provider dashboard

Step 3: Immediate mitigation (5 minutes)
If webhooks are failing but Stripe is operational:

```bash
# Option A: Manually trigger order creation for pending charges
# Get recent successful charges from Stripe
curl https://api.stripe.com/v1/charges?limit=10 \
  -u ${STRIPE_SECRET_KEY}: \
  | jq '.data[] | select(.created > (now - 3600)) | {id: .id, amount: .amount, email: .billing_details.email}'

# Cross-reference against your orders table to find missing orders
# Then manually create orders using backup webhook processor

# Option B: Restart Make.com scenario
curl -X POST "https://api.make.com/v2/scenarios/${SCENARIO_ID}/restart" \
  -H "Authorization: Token ${MAKE_API_KEY}"

# Option C: Enable backup webhook endpoint
# Update Stripe webhook configuration to temporary backup endpoint
curl https://api.stripe.com/v1/webhook_endpoints/${ENDPOINT_ID} \
  -u ${STRIPE_SECRET_KEY}: \
  -d url="https://backup-webhooks.yourstore.com/stripe" \
  -d "enabled=true"
```

Step 4: Root cause investigation (15-30 minutes)
Common failure modes and diagnostics:

Make.com scenario disabled:
```bash
# Check scenario status
curl -X GET "https://api.make.com/v2/scenarios/${SCENARIO_ID}" \
  -H "Authorization: Token ${MAKE_API_KEY}" \
  | jq '{name: .name, status: .status, lastRun: .lastRun}'

# Re-enable if disabled
curl -X PATCH "https://api.make.com/v2/scenarios/${SCENARIO_ID}" \
  -H "Authorization: Token ${MAKE_API_KEY}" \
  -d '{"status": "active"}'
```

Database connection failure:
```sql
-- Check active connections
SELECT
  count(*),
  state,
  wait_event_type,
  wait_event
FROM pg_stat_activity
WHERE datname = current_database()
GROUP BY state, wait_event_type, wait_event;

-- Check for blocking queries
SELECT
  blocked_locks.pid AS blocked_pid,
  blocked_activity.usename AS blocked_user,
  blocking_locks.pid AS blocking_pid,
  blocking_activity.usename AS blocking_user,
  blocked_activity.query AS blocked_statement,
  blocking_activity.query AS blocking_statement
FROM pg_catalog.pg_locks blocked_locks
JOIN pg_catalog.pg_stat_activity blocked_activity ON blocked_activity.pid = blocked_locks.pid
JOIN pg_catalog.pg_locks blocking_locks 
  ON blocking_locks.locktype = blocked_locks.locktype
  AND blocking_locks.relation = blocked_locks.relation
  AND blocking_locks.page = blocked_locks.page
  AND blocking_locks.tuple = blocked_locks.tuple
  AND blocking_locks.pid != blocked_locks.pid
JOIN pg_catalog.pg_stat_activity blocking_activity ON blocking_activity.pid = blocking_locks.pid
WHERE NOT blocked_locks.granted;
```

Webhook authentication failure:
```bash
# Verify webhook signature validation in logs
grep "webhook_signature" /var/log/app/webhooks.log | tail -20

# Rotate webhook secret if compromised
curl https://api.stripe.com/v1/webhook_endpoints/${ENDPOINT_ID}/rotate_secret \
  -u ${STRIPE_SECRET_KEY}: \
  -X POST

# Update new secret in Make.com environment variables
```

Rate limiting:
```sql
-- Check API call frequency
SELECT
  provider_name,
  COUNT(*) AS calls_last_minute,
  COUNT(*) FILTER (WHERE status = 'rate_limited') AS rate_limited_count
FROM api_calls
WHERE created_at > NOW() - INTERVAL '1 minute'
GROUP BY provider_name;
```

Step 5: Restore normal operation (10-20 minutes)
```sql
-- Verify orders flowing again
SELECT
  COUNT(*) AS orders_last_5_min,
  AVG(EXTRACT(EPOCH FROM (completed_at - created_at))) AS avg_completion_seconds
FROM orders
WHERE created_at > NOW() - INTERVAL '5 minutes'
  AND completed_at IS NOT NULL;

-- Check for any orders stuck in processing
SELECT
  order_id,
  stripe_charge_id,
  status,
  created_at,
  EXTRACT(EPOCH FROM (NOW() - created_at)) / 60 AS stuck_minutes
FROM orders
WHERE status IN ('pending', 'processing')
  AND created_at < NOW() - INTERVAL '10 minutes'
ORDER BY created_at;

-- Manually complete stuck orders if needed
UPDATE orders
SET status = 'pending_fulfillment',
    updated_at = NOW()
WHERE order_id IN ('stuck-order-1', 'stuck-order-2');
```

Step 6: Communication (Throughout incident)
```javascript
// Post status updates every 15 minutes during incident
const statusUpdate = {
  incident_id: 'INC-2025-11-16-001',
  status: 'investigating', // or 'mitigating', 'resolved'
  customer_impact: 'Orders cannot be placed',
  eta_resolution: '15 minutes',
  last_update: new Date().toISOString(),
  details: 'Webhook processing restored. Backfilling 8 missed orders from last 12 minutes.'
};

// Post to status page
await fetch('https://status.yourstore.com/api/incidents', {
  method: 'POST',
  headers: {
    'Authorization': `Bearer ${STATUS_PAGE_TOKEN}`,
    'Content-Type': 'application/json'
  },
  body: JSON.stringify(statusUpdate)
});

// Notify internal team
await sendDiscordAlert('HIGH', 'Incident Status Update', JSON.stringify(statusUpdate, null, 2));
```

Step 7: Post-incident review (Within 48 hours)
Document in post-mortem template:
- Timeline of events (detection, actions taken, resolution)
- Root cause analysis (why it happened, not just what failed)
- Customer impact (orders affected, revenue at risk, time to resolution)
- Action items with owners and due dates
- Preventive measures to avoid recurrence

6.3.3 Provider Failure Runbook

Symptom: Orders failing to fulfill with one provider (Printful or Printify),
or high error rates from provider API.

Step 1: Verify provider status (30 seconds)
```sql
-- Check recent fulfillment success rate by provider
SELECT
  provider_name,
  COUNT(*) AS total_attempts,
  COUNT(*) FILTER (WHERE status = 'success') AS successful,
  ROUND(100.0 * COUNT(*) FILTER (WHERE status = 'success') / COUNT(*), 1) AS success_rate,
  COUNT(*) FILTER (WHERE status = 'error') AS errors,
  array_agg(DISTINCT error_code) FILTER (WHERE status = 'error') AS error_codes
FROM fulfillment_events
WHERE created_at > NOW() - INTERVAL '10 minutes'
GROUP BY provider_name;
```

External status pages:
- Printful: https://status.printful.com
- Printify: https://status.printify.com

Step 2: Immediate failover (2 minutes)
```sql
-- Temporarily disable failing provider
UPDATE provider_config
SET is_enabled = false,
    disabled_reason = 'API errors exceeding threshold - manual failover initiated',
    disabled_at = NOW(),
    disabled_by = 'oncall-engineer'
WHERE provider_name = 'printful';

-- Verify fallback provider is operational
SELECT
  provider_name,
  is_enabled,
  last_successful_call,
  current_health_score
FROM provider_config
WHERE provider_name = 'printify';
```

Step 3: Retry failed orders with backup provider (5 minutes)
```javascript
// Get orders that failed with primary provider
const failedOrders = await db.query(`
  SELECT DISTINCT
    o.order_id,
    o.line_items,
    o.customer_email,
    o.shipping_address,
    fe.error_code,
    fe.error_message
  FROM orders o
  JOIN fulfillment_events fe ON o.order_id = fe.order_id
  WHERE fe.provider_name = 'printful'
    AND fe.status = 'error'
    AND fe.created_at > NOW() - INTERVAL '15 minutes'
    AND NOT EXISTS (
      SELECT 1 FROM fulfillment_events fe2
      WHERE fe2.order_id = o.order_id
        AND fe2.provider_name = 'printify'
        AND fe2.status = 'success'
    )
`);

// Submit to backup provider
for (const order of failedOrders) {
  try {
    await submitToPrintify(order);
    console.log(`Resubmitted order ${order.order_id} to Printify`);
  } catch (error) {
    console.error(`Failed to resubmit ${order.order_id}:`, error);
    // Add to manual queue if backup also fails
    await addToManualQueue(order, 'urgent', 'Both providers failed');
  }
}
```

Step 4: Monitor failover effectiveness (Ongoing)
```sql
-- Create view for real-time failover monitoring
CREATE OR REPLACE VIEW failover_status AS
SELECT
  DATE_TRUNC('minute', created_at) AS minute,
  provider_name,
  COUNT(*) AS submission_count,
  COUNT(*) FILTER (WHERE status = 'success') AS successful,
  AVG(response_time_ms) AS avg_response_ms,
  percentile_cont(0.95) WITHIN GROUP (ORDER BY response_time_ms) AS p95_response_ms
FROM fulfillment_events
WHERE created_at > NOW() - INTERVAL '1 hour'
GROUP BY 1, 2
ORDER BY 1 DESC, 2;

-- Query it
SELECT * FROM failover_status WHERE minute > NOW() - INTERVAL '10 minutes';
```

Step 5: Re-enable primary provider when recovered (10 minutes)
```sql
-- Check if primary provider is healthy again
SELECT
  provider_name,
  COUNT(*) AS recent_calls,
  COUNT(*) FILTER (WHERE status = 'success') AS successful,
  ROUND(100.0 * COUNT(*) FILTER (WHERE status = 'success') / COUNT(*), 1) AS success_rate
FROM fulfillment_events
WHERE provider_name = 'printful'
  AND created_at > NOW() - INTERVAL '5 minutes'
GROUP BY provider_name
HAVING COUNT(*) >= 5
  AND COUNT(*) FILTER (WHERE status = 'success')::FLOAT / COUNT(*) >= 0.95;

-- If success rate > 95% for 5+ calls, re-enable
UPDATE provider_config
SET is_enabled = true,
    disabled_reason = NULL,
    disabled_at = NULL,
    re_enabled_at = NOW()
WHERE provider_name = 'printful';
```

Production Reality Box:
┌─────────────────────────────────────────────────────────────────────────────┐
│ PRODUCTION REALITY: Provider Outage During Black Friday                     │
│                                                                             │
│ One store experienced a 2-hour Printful outage on Black Friday that would  │
│ have blocked 347 orders worth $18,450 in revenue. Their automated failover │
│ to Printify completed in 90 seconds, and all orders processed successfully │
│ with the backup provider. When Printful recovered, the system automatically│
│ rebalanced traffic back to normal 80/20 split. Total customer-facing       │
│ impact: zero. Total revenue at risk: zero. This single incident justified  │
│ the entire cost of implementing redundancy ($240 in extra dev time).        │
└─────────────────────────────────────────────────────────────────────────────┘

6.3.4 Database Emergency Procedures

Symptom: Database unreachable, queries timing out, or high connection count.

Step 1: Assess database health (1 minute)
```bash
# Check if database is reachable
pg_isready -h your-db-host.supabase.co -p 5432 -U postgres

# Check connection count
psql -h your-db-host.supabase.co -U postgres -c "
  SELECT count(*), state
  FROM pg_stat_activity
  WHERE datname = 'postgres'
  GROUP BY state;
"

# Check for long-running queries
psql -h your-db-host.supabase.co -U postgres -c "
  SELECT
    pid,
    now() - pg_stat_activity.query_start AS duration,
    query,
    state
  FROM pg_stat_activity
  WHERE (now() - pg_stat_activity.query_start) > interval '5 minutes'
    AND state != 'idle'
  ORDER BY duration DESC;
"
```

Step 2: Immediate mitigation based on diagnosis

If connection pool exhausted:
```sql
-- Kill idle connections (use carefully)
SELECT pg_terminate_backend(pid)
FROM pg_stat_activity
WHERE datname = 'postgres'
  AND state = 'idle'
  AND state_change < NOW() - INTERVAL '10 minutes';

-- Identify and kill runaway queries
SELECT pg_terminate_backend(pid)
FROM pg_stat_activity
WHERE datname = 'postgres'
  AND state = 'active'
  AND query_start < NOW() - INTERVAL '5 minutes'
  AND query NOT LIKE '%pg_stat_activity%';
```

If disk space full:
```sql
-- Check table sizes
SELECT
  schemaname,
  tablename,
  pg_size_pretty(pg_total_relation_size(schemaname||'.'||tablename)) AS size
FROM pg_tables
WHERE schemaname NOT IN ('pg_catalog', 'information_schema')
ORDER BY pg_total_relation_size(schemaname||'.'||tablename) DESC
LIMIT 20;

-- Emergency: Drop oldest log partitions
DROP TABLE IF EXISTS system_logs_2025_10;
DROP TABLE IF EXISTS system_metrics_2025_10;
VACUUM FULL; -- Reclaim space (locks tables, use carefully)
```

If replication lag (Supabase read replicas):
```sql
-- Check replication lag
SELECT
  client_addr,
  state,
  sent_lsn,
  write_lsn,
  flush_lsn,
  replay_lsn,
  sync_state,
  pg_wal_lsn_diff(sent_lsn, replay_lsn) AS replication_lag_bytes
FROM pg_stat_replication;

-- Temporarily disable read replica routing in application
-- Update Make.com or application config to use only primary
```

If cache thrashing or performance degradation:
```sql
-- Check cache hit ratio
SELECT
  sum(heap_blks_read) AS heap_read,
  sum(heap_blks_hit) AS heap_hit,
  sum(heap_blks_hit) / (sum(heap_blks_hit) + sum(heap_blks_read)) AS cache_hit_ratio
FROM pg_statio_user_tables;

-- If cache hit ratio < 0.90, increase shared_buffers or optimize queries

-- Find missing indexes
SELECT
  schemaname,
  tablename,
  seq_scan,
  seq_tup_read,
  idx_scan,
  seq_tup_read / seq_scan AS avg_seq_tup_read
FROM pg_stat_user_tables
WHERE seq_scan > 0
  AND seq_tup_read / seq_scan > 10000
ORDER BY seq_tup_read DESC
LIMIT 10;
```

Step 3: Enable read-only mode if necessary (Last resort)
```sql
-- Prevent writes to preserve database
ALTER DATABASE postgres SET default_transaction_read_only = on;

-- Update application to show maintenance mode message
-- This buys time to resolve issue without data corruption risk
```

Step 4: Restore write capability and verify
```sql
-- Re-enable writes
ALTER DATABASE postgres SET default_transaction_read_only = off;

-- Test write capability
INSERT INTO orders (order_id, customer_email, total_amount, status)
VALUES ('test-' || gen_random_uuid(), 'test@test.com', 100, 'test')
RETURNING order_id;

-- Clean up test data
DELETE FROM orders WHERE customer_email = 'test@test.com' AND status = 'test';
```

6.3.5 Manual Queue Overflow Procedure

Symptom: Manual queue exceeds capacity (50+ items) or urgent items aging > 2 hours.

Step 1: Assess queue state
```sql
-- Get queue statistics
SELECT
  priority,
  COUNT(*) AS pending_count,
  MIN(created_at) AS oldest,
  MAX(created_at) AS newest,
  ROUND(AVG(EXTRACT(EPOCH FROM (NOW() - created_at)) / 60)) AS avg_age_minutes
FROM manual_queue
WHERE status = 'pending'
GROUP BY priority
ORDER BY
  CASE priority
    WHEN 'urgent' THEN 1
    WHEN 'high' THEN 2
    WHEN 'normal' THEN 3
    WHEN 'low' THEN 4
  END;
```

Step 2: Triage and reassign
```javascript
// Automatically reassign items to available team members
async function rebalanceQueue() {
  // Get current queue load per assignee
  const loads = await db.query(`
    SELECT
      assigned_to,
      COUNT(*) AS current_load
    FROM manual_queue
    WHERE status IN ('pending', 'in_progress')
    GROUP BY assigned_to
  `);
  
  // Find assignee with lowest load
  const leastLoaded = loads.sort((a, b) => a.current_load - b.current_load)[0];
  
  // Reassign unassigned urgent items
  await db.query(`
    UPDATE manual_queue
    SET assigned_to = $1,
        updated_at = NOW()
    WHERE status = 'pending'
      AND priority = 'urgent'
      AND assigned_to IS NULL
    RETURNING queue_id, order_id, reason
  `, [leastLoaded.assigned_to]);
}

await rebalanceQueue();
```

Step 3: Notify team for surge support
```javascript
await sendDiscordAlert('HIGH', 'Manual Queue Overflow', `
Manual queue at ${queueDepth} items, above threshold of 50.
${urgentCount} urgent items, oldest is ${oldestAge} minutes old.

Action required:
- All available team members process queue
- Prioritize items marked 'urgent'
- Expected clearance time: ${Math.round(queueDepth / 10)} hours at normal rate

Queue link: https://admin.yourstore.com/manual-queue
`, {
  queue_depth: queueDepth,
  urgent_count: urgentCount,
  oldest_item_age: `${oldestAge} minutes`,
  threshold: 50
});
```

Step 4: Investigate root cause
```sql
-- What's causing queue overflow?
SELECT
  reason,
  COUNT(*) AS occurrence_count,
  ROUND(100.0 * COUNT(*) / SUM(COUNT(*)) OVER (), 1) AS percentage
FROM manual_queue
WHERE created_at > NOW() - INTERVAL '24 hours'
GROUP BY reason
ORDER BY occurrence_count DESC;

-- If specific reason dominates, fix the automation
-- Example: If 80% are "artwork_validation_failed", improve validation rules
```

Validation checkpoint:
  □ Runbooks tested under simulated failure conditions
  □ All team members trained on incident procedures
  □ Escalation contacts verified and up to date
  □ Backup access credentials stored securely and tested
  □ Post-mortem template prepared and accessible
  □ Status page integration functional for customer communication
  □ Database emergency procedures reviewed with DBA if available

═══════════════════════════════════════════════════════════════════════════════

SECTION 6.4: DAILY OPERATIONS PLAYBOOK

Purpose: Maintain system health through consistent routines and proactive
maintenance.

6.4.1 Morning Operations Checklist (15 minutes)

Daily health check routine:
```bash
#!/bin/bash
# daily_health_check.sh - Run every morning at 9am

echo "=== Daily Health Check $(date) ==="

# 1. Check overnight order volume
psql -h ${DB_HOST} -U postgres -c "
  SELECT
    DATE_TRUNC('day', created_at) AS day,
    COUNT(*) AS orders,
    SUM(total_amount) / 100.0 AS revenue_dollars,
    COUNT(*) FILTER (WHERE status = 'failed') AS failed_orders,
    ROUND(100.0 * COUNT(*) FILTER (WHERE status = 'failed') / COUNT(*), 2) AS failure_rate_pct
  FROM orders
  WHERE created_at >= CURRENT_DATE - INTERVAL '1 day'
    AND created_at < CURRENT_DATE
  GROUP BY 1;
"

# 2. Check provider health
psql -h ${DB_HOST} -U postgres -c "
  SELECT
    provider_name,
    COUNT(*) AS submissions,
    ROUND(100.0 * COUNT(*) FILTER (WHERE status = 'success') / COUNT(*), 1) AS success_rate,
    ROUND(AVG(response_time_ms)) AS avg_response_ms
  FROM fulfillment_events
  WHERE created_at >= CURRENT_DATE - INTERVAL '1 day'
  GROUP BY provider_name;
"

# 3. Check active alerts
curl -s "https://api.betteruptime.com/v2/incidents?status=ongoing" \
  -H "Authorization: Bearer ${BETTER_UPTIME_TOKEN}" \
  | jq '.data[] | {name: .attributes.name, started: .attributes.started_at}'

# 4. Check manual queue
psql -h ${DB_HOST} -U postgres -c "
  SELECT
    COUNT(*) AS pending_items,
    COUNT(*) FILTER (WHERE priority = 'urgent') AS urgent_items,
    MAX(EXTRACT(EPOCH FROM (NOW() - created_at)) / 60)::INTEGER AS oldest_age_minutes
  FROM manual_queue
  WHERE status = 'pending';
"

# 5. Check database size growth
psql -h ${DB_HOST} -U postgres -c "
  SELECT
    pg_size_pretty(pg_database_size(current_database())) AS database_size,
    pg_size_pretty(pg_total_relation_size('orders')) AS orders_table_size,
    pg_size_pretty(pg_total_relation_size('system_logs')) AS logs_table_size;
"

# 6. Check error patterns overnight
psql -h ${DB_HOST} -U postgres -c "
  SELECT
    error_code,
    COUNT(*) AS occurrences
  FROM system_logs
  WHERE level = 'error'
    AND timestamp >= CURRENT_DATE - INTERVAL '1 day'
  GROUP BY error_code
  ORDER BY occurrences DESC
  LIMIT 10;
"

echo "=== Health Check Complete ==="
```

Automated execution via Make.com:
```javascript
// Schedule daily health check
const healthCheckResults = await executeShellCommand('bash /scripts/daily_health_check.sh');

// Parse results and send to Discord
await sendDiscordMessage({
  channel: 'daily-reports',
  content: '**Daily Health Report**\n```' + healthCheckResults + '```',
  mentions: healthCheckResults.includes('CRITICAL') ? ['@oncall'] : []
});

// Store results in database
await db.query(`
  INSERT INTO daily_health_reports (report_date, report_content, status)
  VALUES (CURRENT_DATE, $1, $2)
`, [healthCheckResults, healthCheckResults.includes('CRITICAL') ? 'needs_attention' : 'healthy']);
```

6.4.2 Weekly Maintenance Tasks (1-2 hours)

Sunday evening or Monday morning routine:

Task 1: Database maintenance (30 minutes)
```sql
-- Vacuum and analyze all tables
VACUUM ANALYZE orders;
VACUUM ANALYZE fulfillment_events;
VACUUM ANALYZE system_logs;
VACUUM ANALYZE system_metrics;

-- Rebuild fragmented indexes
REINDEX TABLE CONCURRENTLY orders;
REINDEX TABLE CONCURRENTLY fulfillment_events;

-- Check for missing indexes on frequently queried columns
SELECT
  schemaname,
  tablename,
  attname,
  n_distinct,
  correlation
FROM pg_stats
WHERE schemaname = 'public'
  AND n_distinct > 100
  AND correlation < 0.5
ORDER BY tablename, attname;

-- Update table statistics
ANALYZE VERBOSE;
```

Task 2: Log and metric retention (15 minutes)
```sql
-- Drop old log partitions (keep last 90 days)
DO $$
DECLARE
  partition_name TEXT;
BEGIN
  FOR partition_name IN
    SELECT tablename
    FROM pg_tables
    WHERE schemaname = 'public'
      AND tablename LIKE 'system_logs_%'
      AND tablename < 'system_logs_' || TO_CHAR(CURRENT_DATE - INTERVAL '90 days', 'YYYY_MM')
  LOOP
    EXECUTE format('DROP TABLE IF EXISTS %I CASCADE', partition_name);
    RAISE NOTICE 'Dropped partition: %', partition_name;
  END LOOP;
END $$;

-- Same for metrics
DO $$
DECLARE
  partition_name TEXT;
BEGIN
  FOR partition_name IN
    SELECT tablename
    FROM pg_tables
    WHERE schemaname = 'public'
      AND tablename LIKE 'system_metrics_%'
      AND tablename < 'system_metrics_' || TO_CHAR(CURRENT_DATE - INTERVAL '90 days', 'YYYY_MM')
  LOOP
    EXECUTE format('DROP TABLE IF EXISTS %I CASCADE', partition_name);
    RAISE NOTICE 'Dropped partition: %', partition_name;
  END LOOP;
END $$;

-- Verify retention working
SELECT
  tablename,
  pg_size_pretty(pg_total_relation_size('public.' || tablename)) AS size
FROM pg_tables
WHERE schemaname = 'public'
  AND (tablename LIKE 'system_logs_%' OR tablename LIKE 'system_metrics_%')
ORDER BY tablename DESC;
```

Task 3: Cost review (20 minutes)
```sql
-- Generate weekly cost report
WITH weekly_costs AS (
  SELECT
    DATE_TRUNC('week', created_at) AS week,
    provider_name,
    SUM(cost_cents + shipping_cost_cents) AS total_cost_cents,
    COUNT(*) AS order_count,
    AVG(cost_cents + shipping_cost_cents) AS avg_cost_cents
  FROM fulfillment_events
  WHERE status = 'submitted'
    AND created_at >= CURRENT_DATE - INTERVAL '8 weeks'
  GROUP BY 1, 2
)
SELECT
  week,
  provider_name,
  ROUND(total_cost_cents / 100.0, 2) AS total_cost,
  order_count,
  ROUND(avg_cost_cents / 100.0, 2) AS avg_cost_per_order,
  ROUND(100.0 * (total_cost_cents - LAG(total_cost_cents) OVER (PARTITION BY provider_name ORDER BY week)) / 
    NULLIF(LAG(total_cost_cents) OVER (PARTITION BY provider_name ORDER BY week), 0), 1) AS week_over_week_change_pct
FROM weekly_costs
ORDER BY week DESC, provider_name;

-- Check for cost anomalies
SELECT
  provider_name,
  DATE(created_at) AS day,
  COUNT(*) AS orders,
  SUM(cost_cents + shipping_cost_cents) / 100.0 AS daily_cost,
  AVG(cost_cents + shipping_cost_cents) / 100.0 AS avg_cost_per_order
FROM fulfillment_events
WHERE created_at >= CURRENT_DATE - INTERVAL '7 days'
  AND status = 'submitted'
GROUP BY provider_name, DATE(created_at)
HAVING AVG(cost_cents + shipping_cost_cents) > (
  SELECT AVG(cost_cents + shipping_cost_cents) * 1.2
  FROM fulfillment_events
  WHERE created_at >= CURRENT_DATE - INTERVAL '30 days'
    AND status = 'submitted'
)
ORDER BY daily_cost DESC;
```

Task 4: Security audit (15 minutes)
```sql
-- Check for suspicious activity patterns
-- 1. Unusual order volumes from single email
SELECT
  customer_email,
  COUNT(*) AS order_count,
  SUM(total_amount) / 100.0 AS total_spent,
  MIN(created_at) AS first_order,
  MAX(created_at) AS last_order
FROM orders
WHERE created_at >= CURRENT_DATE - INTERVAL '7 days'
GROUP BY customer_email
HAVING COUNT(*) > 10
ORDER BY order_count DESC;

-- 2. Failed payment attempts from same IP
SELECT
  ip_address,
  COUNT(*) AS failed_attempts,
  COUNT(DISTINCT customer_email) AS unique_emails,
  array_agg(DISTINCT error_code) AS error_codes
FROM payment_attempts
WHERE status = 'failed'
  AND created_at >= CURRENT_DATE - INTERVAL '7 days'
GROUP BY ip_address
HAVING COUNT(*) > 5
ORDER BY failed_attempts DESC;

-- 3. Check for rate limit violations
SELECT
  service,
  COUNT(*) AS rate_limit_hits,
  array_agg(DISTINCT ip_address) AS source_ips
FROM system_logs
WHERE message LIKE '%rate limit%'
  AND timestamp >= CURRENT_DATE - INTERVAL '7 days'
GROUP BY service;
```

Task 5: Performance optimization review (20 minutes)
```sql
-- Find slow queries
SELECT
  calls,
  ROUND(mean_exec_time::NUMERIC, 2) AS avg_ms,
  ROUND(total_exec_time::NUMERIC, 2) AS total_ms,
  ROUND((100 * total_exec_time / SUM(total_exec_time) OVER ())::NUMERIC, 2) AS pct_total_time,
  query
FROM pg_stat_statements
WHERE calls > 100
ORDER BY total_exec_time DESC
LIMIT 20;

-- Check index usage
SELECT
  schemaname,
  tablename,
  indexname,
  idx_scan AS index_scans,
  idx_tup_read AS tuples_read,
  idx_tup_fetch AS tuples_fetched,
  pg_size_pretty(pg_relation_size(indexrelid)) AS index_size
FROM pg_stat_user_indexes
WHERE idx_scan = 0
  AND indexrelname NOT LIKE '%pkey%'
ORDER BY pg_relation_size(indexrelid) DESC;

-- Consider dropping unused indexes (carefully)
-- DROP INDEX CONCURRENTLY index_name; -- Only after confirming it's truly unused
```

6.4.3 Monthly Strategic Reviews (2-3 hours)

First business day of each month:

Review 1: Business metrics analysis (45 minutes)
```sql
-- Month-over-month growth
WITH monthly_stats AS (
  SELECT
    DATE_TRUNC('month', created_at) AS month,
    COUNT(*) AS orders,
    COUNT(DISTINCT customer_email) AS unique_customers,
    SUM(total_amount) / 100.0 AS revenue,
    AVG(total_amount) / 100.0 AS avg_order_value
  FROM orders
  WHERE created_at >= CURRENT_DATE - INTERVAL '12 months'
    AND status NOT IN ('cancelled', 'failed')
  GROUP BY 1
)
SELECT
  month,
  orders,
  unique_customers,
  ROUND(revenue, 2) AS revenue,
  ROUND(avg_order_value, 2) AS aov,
  ROUND(100.0 * (orders - LAG(orders) OVER (ORDER BY month)) / LAG(orders) OVER (ORDER BY month), 1) AS order_growth_pct,
  ROUND(100.0 * (revenue - LAG(revenue) OVER (ORDER BY month)) / LAG(revenue) OVER (ORDER BY month), 1) AS revenue_growth_pct
FROM monthly_stats
ORDER BY month DESC;

-- Customer cohort analysis
WITH first_purchase AS (
  SELECT
    customer_email,
    DATE_TRUNC('month', MIN(created_at)) AS cohort_month
  FROM orders
  WHERE status NOT IN ('cancelled', 'failed')
  GROUP BY customer_email
),
cohort_orders AS (
  SELECT
    fp.cohort_month,
    DATE_TRUNC('month', o.created_at) AS order_month,
    COUNT(DISTINCT o.customer_email) AS customers,
    SUM(o.total_amount) / 100.0 AS revenue
  FROM first_purchase fp
  JOIN orders o ON fp.customer_email = o.customer_email
  WHERE o.status NOT IN ('cancelled', 'failed')
  GROUP BY 1, 2
)
SELECT
  cohort_month,
  order_month,
  customers,
  ROUND(revenue, 2) AS revenue,
  ROUND(100.0 * customers / FIRST_VALUE(customers) OVER (PARTITION BY cohort_month ORDER BY order_month), 1) AS retention_pct
FROM cohort_orders
WHERE cohort_month >= CURRENT_DATE - INTERVAL '12 months'
ORDER BY cohort_month DESC, order_month;
```

Review 2: System reliability report (30 minutes)
```sql
-- Calculate monthly SLOs
WITH monthly_availability AS (
  SELECT
    DATE_TRUNC('month', timestamp) AS month,
    COUNT(*) FILTER (WHERE metric_name = 'system_available' AND metric_value = 1) AS available_minutes,
    COUNT(*) FILTER (WHERE metric_name = 'system_available') AS total_minutes
  FROM system_metrics
  WHERE metric_name = 'system_available'
    AND timestamp >= CURRENT_DATE - INTERVAL '12 months'
  GROUP BY 1
)
SELECT
  month,
  ROUND(100.0 * available_minutes / NULLIF(total_minutes, 0), 4) AS availability_pct,
  total_minutes - available_minutes AS downtime_minutes,
  ROUND((total_minutes - available_minutes) / 60.0, 1) AS downtime_hours
FROM monthly_availability
ORDER BY month DESC;

-- Error budget consumption
WITH error_budget AS (
  SELECT
    DATE_TRUNC('month', created_at) AS month,
    COUNT(*) AS total_requests,
    COUNT(*) FILTER (WHERE status IN ('error', 'failed')) AS failed_requests,
    ROUND(100.0 * COUNT(*) FILTER (WHERE status IN ('error', 'failed')) / COUNT(*), 2) AS error_rate_pct
  FROM orders
  WHERE created_at >= CURRENT_DATE - INTERVAL '12 months'
  GROUP BY 1
)
SELECT
  month,
  total_requests,
  failed_requests,
  error_rate_pct,
  CASE
    WHEN error_rate_pct <= 1.0 THEN 'Within Budget'
    WHEN error_rate_pct <= 2.0 THEN 'Warning'
    ELSE 'Budget Exceeded'
  END AS budget_status
FROM error_budget
ORDER BY month DESC;
```

Review 3: Cost optimization opportunities (45 minutes)
```javascript
// Run cost analysis script
const costAnalysis = {
  fulfillment: await analyzeFulfillmentCosts(),
  infrastructure: await analyzeInfrastructureCosts(),
  apis: await analyzeApiCosts()
};

async function analyzeFulfillmentCosts() {
  const results = await db.query(`
    WITH provider_comparison AS (
      SELECT
        provider_name,
        product_category,
        COUNT(*) AS order_count,
        AVG(cost_cents + shipping_cost_cents) AS avg_total_cost,
        STDDEV(cost_cents + shipping_cost_cents) AS cost_stddev,
        percentile_cont(0.5) WITHIN GROUP (ORDER BY cost_cents + shipping_cost_cents) AS median_cost
      FROM fulfillment_events
      WHERE created_at >= CURRENT_DATE - INTERVAL '30 days'
        AND status = 'submitted'
      GROUP BY provider_name, product_category
    )
    SELECT
      product_category,
      json_object_agg(provider_name, json_build_object(
        'avg_cost', ROUND(avg_total_cost / 100.0, 2),
        'median_cost', ROUND(median_cost / 100.0, 2),
        'order_count', order_count
      )) AS provider_costs,
      ROUND((MAX(avg_total_cost) - MIN(avg_total_cost)) / 100.0, 2) AS potential_savings_per_order
    FROM provider_comparison
    GROUP BY product_category
    HAVING COUNT(DISTINCT provider_name) > 1
  `);
  
  return results.rows;
}

// Generate recommendations
const recommendations = [];

// Check if shifting product mix between providers could save money
for (const category of costAnalysis.fulfillment) {
  if (category.potential_savings_per_order > 1.00) {
    recommendations.push({
      type: 'fulfillment_optimization',
      category: category.product_category,
      estimated_monthly_savings: category.potential_savings_per_order * monthlyOrderCount,
      action: `Consider shifting ${category.product_category} orders to lower-cost provider`
    });
  }
}

// Check if database plan can be downgraded
const dbSize = await getDatabaseSize();
if (dbSize < currentPlanLimit * 0.5) {
  recommendations.push({
    type: 'infrastructure_optimization',
    resource: 'database',
    estimated_monthly_savings: 25,
    action: 'Current database usage only 45% of plan capacity - consider downgrading tier'
  });
}

// Generate report
await generateCostOptimizationReport(recommendations);
```

Review 4: Capacity planning (30 minutes)
```sql
-- Trend analysis for capacity planning
WITH daily_volume AS (
  SELECT
    DATE(created_at) AS day,
    COUNT(*) AS orders,
    MAX(COUNT(*)) OVER (ORDER BY DATE(created_at) ROWS BETWEEN 7 PRECEDING AND CURRENT ROW) AS peak_orders_7d
  FROM orders
  WHERE created_at >= CURRENT_DATE - INTERVAL '90 days'
  GROUP BY DATE(created_at)
)
SELECT
  AVG(orders) AS avg_daily_orders,
  MAX(orders) AS peak_daily_orders,
  AVG(peak_orders_7d) AS avg_7d_peak,
  percentile_cont(0.95) WITHIN GROUP (ORDER BY orders) AS p95_daily_orders,
  -- Extrapolate to estimate capacity needs 3 months out
  AVG(orders) * 1.5 AS projected_avg_3mo_out,
  MAX(orders) * 1.5 AS projected_peak_3mo_out
FROM daily_volume;

-- Database growth rate
WITH monthly_growth AS (
  SELECT
    DATE_TRUNC('month', NOW()) AS month,
    pg_database_size(current_database()) AS current_size,
    LAG(pg_database_size(current_database())) OVER (ORDER BY DATE_TRUNC('month', NOW())) AS previous_size
  FROM generate_series(CURRENT_DATE - INTERVAL '6 months', CURRENT_DATE, '1 month') AS months
)
SELECT
  month,
  pg_size_pretty(current_size) AS size,
  pg_size_pretty(current_size - previous_size) AS growth,
  ROUND(100.0 * (current_size - previous_size) / NULLIF(previous_size, 0), 1) AS growth_pct,
  -- Project 6 months out
  pg_size_pretty(current_size + (current_size - previous_size) * 6) AS projected_6mo_size
FROM monthly_growth
WHERE previous_size IS NOT NULL;
```

Production Reality Box:
┌─────────────────────────────────────────────────────────────────────────────┐
│ PRODUCTION REALITY: Neglected Daily Checks Cost $4,200                      │
│                                                                             │
│ One team skipped daily operations checks for 3 weeks during a busy season. │
│ They missed that their database had grown to 98% capacity, which caused a  │
│ catastrophic outage when it hit 100% during peak traffic. The outage       │
│ lasted 6 hours (time to provision larger database and restore from backup).│
│ 347 orders were lost, customers complained publicly, and the direct        │
│ revenue loss was $18,450. The indirect reputation damage was immeasurable. │
│ After implementing automated daily checks (15 minutes/day), they've had    │
│ zero similar incidents in 18 months. Time invested: 90 hours/year.         │
│ Incidents prevented: countless. ROI: infinite.                             │
└─────────────────────────────────────────────────────────────────────────────┘

Validation checkpoint:
  □ Daily health check script runs automatically every morning
  □ Weekly maintenance tasks scheduled and completed consistently
  □ Monthly reviews generate actionable insights and recommendations
  □ All operations procedures documented and accessible to team
  □ Backup operator trained and capable of executing all routines
  □ Metrics tracked over time to measure operational improvements

═══════════════════════════════════════════════════════════════════════════════



SECTION 6.5: PERFORMANCE TUNING AND OPTIMIZATION

Purpose: Maintain fast response times and efficient resource usage as order
volume scales.

6.5.1 Query Performance Analysis

Identify and optimize slow queries:
```sql
-- Enable query statistics collection (if not already enabled)
CREATE EXTENSION IF NOT EXISTS pg_stat_statements;

-- Find queries consuming most total time
SELECT
  calls,
  ROUND(mean_exec_time::NUMERIC, 2) AS avg_ms,
  ROUND(total_exec_time::NUMERIC, 2) AS total_ms,
  ROUND((100 * total_exec_time / SUM(total_exec_time) OVER ())::NUMERIC, 2) AS pct_time,
  LEFT(query, 100) AS query_preview
FROM pg_stat_statements
WHERE query NOT LIKE '%pg_stat_statements%'
ORDER BY total_exec_time DESC
LIMIT 20;

-- Find queries with highest average execution time
SELECT
  calls,
  ROUND(mean_exec_time::NUMERIC, 2) AS avg_ms,
  ROUND(max_exec_time::NUMERIC, 2) AS max_ms,
  ROUND(stddev_exec_time::NUMERIC, 2) AS stddev_ms,
  LEFT(query, 100) AS query_preview
FROM pg_stat_statements
WHERE calls > 10
  AND query NOT LIKE '%pg_stat_statements%'
ORDER BY mean_exec_time DESC
LIMIT 20;
```

Query optimization example - Before:
```sql
-- SLOW: Sequential scan on large table (avg 2,400ms for 100K rows)
SELECT
  o.order_id,
  o.customer_email,
  o.total_amount,
  o.created_at,
  f.provider_name,
  f.status AS fulfillment_status
FROM orders o
LEFT JOIN fulfillment_events f ON o.order_id = f.order_id
WHERE o.customer_email = 'customer@example.com'
ORDER BY o.created_at DESC;

-- Query plan shows:
-- Seq Scan on orders (cost=0.00..4523.15 rows=15 width=120) (actual time=2401.234)
--   Filter: (customer_email = 'customer@example.com')
--   Rows Removed by Filter: 99985
```

Query optimization example - After:
```sql
-- FAST: Index scan with covering index (avg 12ms)
CREATE INDEX CONCURRENTLY idx_orders_customer_email_created_at
  ON orders(customer_email, created_at DESC)
  INCLUDE (order_id, total_amount);

-- Same query now uses index:
-- Index Scan using idx_orders_customer_email_created_at (cost=0.42..45.67 rows=15 width=120) (actual time=11.523)
--   Index Cond: (customer_email = 'customer@example.com')
```

Composite index for common filters:
```sql
-- Orders often filtered by status and date range
CREATE INDEX CONCURRENTLY idx_orders_status_created
  ON orders(status, created_at DESC)
  WHERE status IN ('pending_fulfillment', 'processing');

-- Partial index for active orders only (smaller, faster)
CREATE INDEX CONCURRENTLY idx_orders_active
  ON orders(created_at DESC)
  WHERE status NOT IN ('completed', 'cancelled', 'failed');

-- Fulfillment events by provider and status
CREATE INDEX CONCURRENTLY idx_fulfillment_provider_status_created
  ON fulfillment_events(provider_name, status, created_at DESC);
```

6.5.2 Database Connection Pooling

Optimize connections with PgBouncer or Supabase built-in pooling:
```javascript
// Make.com HTTP module for database queries
// Configure connection pooling settings

const poolConfig = {
  // Connection string with pooling enabled
  connectionString: process.env.DATABASE_URL + '?pgbouncer=true',
  
  // Pool settings
  max: 20, // Maximum connections in pool
  min: 5,  // Minimum idle connections
  idleTimeoutMillis: 30000,
  connectionTimeoutMillis: 2000,
  
  // Statement timeout to prevent runaway queries
  statement_timeout: 30000, // 30 seconds
  
  // SSL settings
  ssl: {
    rejectUnauthorized: true
  }
};

// Connection handling with retry logic
async function executeQuery(query, params, retries = 3) {
  for (let attempt = 1; attempt <= retries; attempt++) {
    try {
      const client = await pool.connect();
      try {
        const result = await client.query(query, params);
        return result.rows;
      } finally {
        client.release(); // Always release back to pool
      }
    } catch (error) {
      if (attempt === retries) throw error;
      
      // Wait before retry with exponential backoff
      await new Promise(resolve => setTimeout(resolve, Math.pow(2, attempt) * 1000));
      console.log(`Query retry attempt ${attempt + 1}/${retries}`);
    }
  }
}
```

Connection pool monitoring:
```sql
-- Monitor pool usage
SELECT
  count(*) AS total_connections,
  count(*) FILTER (WHERE state = 'active') AS active,
  count(*) FILTER (WHERE state = 'idle') AS idle,
  count(*) FILTER (WHERE state = 'idle in transaction') AS idle_in_transaction,
  max(now() - query_start) AS longest_query_duration
FROM pg_stat_activity
WHERE datname = current_database();

-- Alert if idle in transaction connections exist (connection leaks)
SELECT
  pid,
  usename,
  application_name,
  client_addr,
  state,
  query_start,
  state_change,
  wait_event_type,
  wait_event,
  LEFT(query, 100) AS query_preview
FROM pg_stat_activity
WHERE state = 'idle in transaction'
  AND state_change < NOW() - INTERVAL '5 minutes';

-- Kill problematic idle in transaction connections
SELECT pg_terminate_backend(pid)
FROM pg_stat_activity
WHERE state = 'idle in transaction'
  AND state_change < NOW() - INTERVAL '10 minutes';
```

6.5.3 Caching Strategy

Implement multi-layer caching for frequently accessed data:

Application-level cache (in Make.com scenario):
```javascript
// Simple in-memory cache with TTL
class SimpleCache {
  constructor() {
    this.cache = new Map();
  }
  
  set(key, value, ttlSeconds = 300) {
    this.cache.set(key, {
      value: value,
      expiresAt: Date.now() + (ttlSeconds * 1000)
    });
  }
  
  get(key) {
    const item = this.cache.get(key);
    if (!item) return null;
    
    if (Date.now() > item.expiresAt) {
      this.cache.delete(key);
      return null;
    }
    
    return item.value;
  }
  
  invalidate(key) {
    this.cache.delete(key);
  }
  
  clear() {
    this.cache.clear();
  }
}

const cache = new SimpleCache();

// Cache provider configuration (rarely changes)
async function getProviderConfig(providerName) {
  const cacheKey = `provider_config_${providerName}`;
  let config = cache.get(cacheKey);
  
  if (!config) {
    config = await db.query(`
      SELECT * FROM provider_config WHERE provider_name = $1
    `, [providerName]);
    
    cache.set(cacheKey, config, 600); // Cache for 10 minutes
  }
  
  return config;
}

// Cache product catalog
async function getProductCatalog(providerName) {
  const cacheKey = `product_catalog_${providerName}`;
  let catalog = cache.get(cacheKey);
  
  if (!catalog) {
    catalog = await fetchFromProvider(providerName, '/products');
    cache.set(cacheKey, catalog, 3600); // Cache for 1 hour
  }
  
  return catalog;
}

// Invalidate cache when configuration changes
async function updateProviderConfig(providerName, newConfig) {
  await db.query(`
    UPDATE provider_config SET ... WHERE provider_name = $1
  `, [providerName]);
  
  // Invalidate cached config
  cache.invalidate(`provider_config_${providerName}`);
}
```

Database-level materialized views for analytics:
```sql
-- Create materialized view for daily order summary
CREATE MATERIALIZED VIEW mv_daily_order_summary AS
SELECT
  DATE(created_at) AS order_date,
  status,
  COUNT(*) AS order_count,
  SUM(total_amount) AS total_revenue,
  AVG(total_amount) AS avg_order_value,
  COUNT(DISTINCT customer_email) AS unique_customers
FROM orders
GROUP BY DATE(created_at), status;

CREATE UNIQUE INDEX ON mv_daily_order_summary (order_date, status);

-- Refresh materialized view (run daily)
REFRESH MATERIALIZED VIEW CONCURRENTLY mv_daily_order_summary;

-- Now queries are instant instead of scanning full orders table
SELECT * FROM mv_daily_order_summary
WHERE order_date >= CURRENT_DATE - INTERVAL '90 days'
ORDER BY order_date DESC;

-- Automate refresh via pg_cron or Make.com scheduled scenario
-- CREATE EXTENSION pg_cron;
-- SELECT cron.schedule('refresh-daily-summary', '0 2 * * *', $$
--   REFRESH MATERIALIZED VIEW CONCURRENTLY mv_daily_order_summary;
-- $$);
```

Redis caching for high-frequency data (if needed at scale):
```javascript
// Redis cache integration (optional, for > 1000 orders/day)
const Redis = require('ioredis');
const redis = new Redis(process.env.REDIS_URL);

async function getCachedOrCompute(key, computeFn, ttlSeconds = 300) {
  // Try to get from cache
  const cached = await redis.get(key);
  if (cached) {
    return JSON.parse(cached);
  }
  
  // Compute value
  const value = await computeFn();
  
  // Store in cache
  await redis.setex(key, ttlSeconds, JSON.stringify(value));
  
  return value;
}

// Usage for expensive query
const dashboardData = await getCachedOrCompute(
  'dashboard:last_24_hours',
  async () => {
    return await db.query(`
      SELECT
        COUNT(*) AS orders,
        SUM(total_amount) / 100.0 AS revenue,
        -- ... other metrics
      FROM orders
      WHERE created_at > NOW() - INTERVAL '24 hours'
    `);
  },
  60 // Cache for 1 minute
);
```

6.5.4 Batch Processing for Efficiency

Process operations in batches to reduce overhead:

Batch order status updates:
```javascript
// INEFFICIENT: Update orders one at a time
for (const orderId of orderIds) {
  await db.query(`
    UPDATE orders SET status = 'completed' WHERE order_id = $1
  `, [orderId]);
}
// Time for 100 orders: ~3,500ms (35ms per query)

// EFFICIENT: Batch update
await db.query(`
  UPDATE orders
  SET status = 'completed', updated_at = NOW()
  WHERE order_id = ANY($1)
`, [orderIds]);
// Time for 100 orders: ~120ms (total)
```

Batch inserts for metrics:
```javascript
// Collect metrics for 1 minute, then batch insert
const metricsBuffer = [];
const BATCH_SIZE = 100;
const FLUSH_INTERVAL_MS = 60000;

function recordMetric(metricName, metricValue, tags = {}) {
  metricsBuffer.push({
    metric_name: metricName,
    metric_value: metricValue,
    tags: tags,
    timestamp: new Date()
  });
  
  if (metricsBuffer.length >= BATCH_SIZE) {
    flushMetrics();
  }
}

async function flushMetrics() {
  if (metricsBuffer.length === 0) return;
  
  const metrics = metricsBuffer.splice(0, metricsBuffer.length);
  
  // Batch insert
  const values = metrics.map((m, idx) => 
    `($${idx*4+1}, $${idx*4+2}, $${idx*4+3}, $${idx*4+4})`
  ).join(',');
  
  const params = metrics.flatMap(m => [
    m.metric_name,
    m.metric_value,
    JSON.stringify(m.tags),
    m.timestamp
  ]);
  
  await db.query(`
    INSERT INTO system_metrics (metric_name, metric_value, tags, timestamp)
    VALUES ${values}
  `, params);
}

// Flush on interval
setInterval(flushMetrics, FLUSH_INTERVAL_MS);
```

Bulk provider API calls:
```javascript
// Get shipping rates for multiple orders at once
async function bulkGetShippingRates(orders) {
  // Group orders by provider to batch API calls
  const byProvider = orders.reduce((acc, order) => {
    const provider = selectProvider(order);
    if (!acc[provider]) acc[provider] = [];
    acc[provider].push(order);
    return acc;
  }, {});
  
  // Call each provider once with all orders
  const results = await Promise.all(
    Object.entries(byProvider).map(async ([provider, providerOrders]) => {
      if (provider === 'printful') {
        return await printfulBulkShippingRate(providerOrders);
      } else {
        return await printifyBulkShippingRate(providerOrders);
      }
    })
  );
  
  return results.flat();
}

// Printful supports batch shipping rate calculation
async function printfulBulkShippingRate(orders) {
  const response = await fetch('https://api.printful.com/shipping/rates', {
    method: 'POST',
    headers: {
      'Authorization': `Bearer ${PRINTFUL_API_KEY}`,
      'Content-Type': 'application/json'
    },
    body: JSON.stringify({
      recipient: orders[0].shipping_address, // Assuming same destination
      items: orders.flatMap(o => o.line_items.map(item => ({
        variant_id: item.variant_id,
        quantity: item.quantity
      })))
    })
  });
  
  return await response.json();
}
```

6.5.5 API Rate Limit Management

Implement smart rate limiting to avoid throttling:

```javascript
// Token bucket rate limiter
class RateLimiter {
  constructor(maxTokens, refillRate, refillInterval) {
    this.maxTokens = maxTokens;
    this.tokens = maxTokens;
    this.refillRate = refillRate;
    this.refillInterval = refillInterval;
    
    setInterval(() => this.refill(), refillInterval);
  }
  
  refill() {
    this.tokens = Math.min(this.maxTokens, this.tokens + this.refillRate);
  }
  
  async consume(tokens = 1) {
    while (this.tokens < tokens) {
      // Wait for refill
      await new Promise(resolve => setTimeout(resolve, 100));
    }
    
    this.tokens -= tokens;
    return true;
  }
  
  available() {
    return this.tokens;
  }
}

// Provider-specific rate limiters
const rateLimiters = {
  printful: new RateLimiter(120, 2, 1000), // 120 requests/minute
  printify: new RateLimiter(600, 10, 1000), // 600 requests/minute
  stripe: new RateLimiter(100, 2, 1000) // 100 requests/second (very generous limit)
};

// Wrap API calls with rate limiting
async function callProviderAPI(provider, endpoint, options) {
  await rateLimiters[provider].consume();
  
  const startTime = Date.now();
  try {
    const response = await fetch(endpoint, options);
    
    // Check for rate limit headers
    const remaining = response.headers.get('X-RateLimit-Remaining');
    const resetTime = response.headers.get('X-RateLimit-Reset');
    
    if (remaining && parseInt(remaining) < 10) {
      console.warn(`${provider} rate limit approaching: ${remaining} requests remaining`);
    }
    
    if (response.status === 429) {
      // Rate limited - wait and retry
      const retryAfter = response.headers.get('Retry-After') || 60;
      console.log(`Rate limited by ${provider}, waiting ${retryAfter} seconds`);
      await new Promise(resolve => setTimeout(resolve, retryAfter * 1000));
      return await callProviderAPI(provider, endpoint, options);
    }
    
    // Record metrics
    await recordMetric('api_call_duration_ms', Date.now() - startTime, {
      provider: provider,
      status: response.status
    });
    
    return response;
  } catch (error) {
    await recordMetric('api_call_error', 1, {
      provider: provider,
      error: error.message
    });
    throw error;
  }
}
```

Request queue with priority:
```javascript
// Priority queue for API requests
class PriorityQueue {
  constructor() {
    this.queues = {
      urgent: [],
      high: [],
      normal: [],
      low: []
    };
    this.processing = false;
  }
  
  enqueue(request, priority = 'normal') {
    this.queues[priority].push(request);
    this.process();
  }
  
  async process() {
    if (this.processing) return;
    this.processing = true;
    
    while (this.hasRequests()) {
      const request = this.dequeue();
      if (request) {
        try {
          await request.execute();
        } catch (error) {
          console.error('Request failed:', error);
          if (request.retryable && request.retries < 3) {
            request.retries = (request.retries || 0) + 1;
            this.enqueue(request, 'high'); // Retry with high priority
          }
        }
      }
    }
    
    this.processing = false;
  }
  
  dequeue() {
    // Process in priority order
    for (const priority of ['urgent', 'high', 'normal', 'low']) {
      if (this.queues[priority].length > 0) {
        return this.queues[priority].shift();
      }
    }
    return null;
  }
  
  hasRequests() {
    return Object.values(this.queues).some(q => q.length > 0);
  }
}

const requestQueue = new PriorityQueue();

// Usage
requestQueue.enqueue({
  execute: async () => {
    return await submitOrderToProvider(orderId, 'printful');
  },
  retryable: true
}, 'high');
```

6.5.6 Frontend Performance (If applicable)

Optimize storefront or admin dashboard:

Lazy load images:
```html
<!-- Use native lazy loading -->
<img src="product-image.jpg" loading="lazy" alt="Product Name">

<!-- Or intersection observer for older browsers -->
<img data-src="product-image.jpg" class="lazy" alt="Product Name">

<script>
const lazyImages = document.querySelectorAll('img.lazy');
const imageObserver = new IntersectionObserver((entries) => {
  entries.forEach(entry => {
    if (entry.isIntersecting) {
      const img = entry.target;
      img.src = img.dataset.src;
      img.classList.remove('lazy');
      imageObserver.unobserve(img);
    }
  });
});

lazyImages.forEach(img => imageObserver.observe(img));
</script>
```

Minimize JavaScript bundle:
```javascript
// Code splitting - load features on demand
const AdminDashboard = React.lazy(() => import('./AdminDashboard'));
const OrderHistory = React.lazy(() => import('./OrderHistory'));

function App() {
  return (
    <Suspense fallback={<div>Loading...</div>}>
      <Router>
        <Route path="/admin" element={<AdminDashboard />} />
        <Route path="/orders" element={<OrderHistory />} />
      </Router>
    </Suspense>
  );
}
```

Use CDN for static assets:
```html
<!-- Serve images from CDN -->
<img src="https://cdn.yourstore.com/products/tshirt-001.jpg" alt="T-Shirt">

<!-- Configure cache headers -->
Cache-Control: public, max-age=31536000, immutable
```

Production Reality Box:
┌─────────────────────────────────────────────────────────────────────────────┐
│ PRODUCTION REALITY: Query Optimization Saved 11 Seconds Per Order           │
│                                                                             │
│ One store had a dashboard query that scanned the entire orders table every │
│ time an admin viewed it. With 50,000 orders, this took 11.2 seconds. By    │
│ adding a composite index on (status, created_at) and creating a            │
│ materialized view for daily summaries, query time dropped to 180ms - a 62x │
│ improvement. Admin satisfaction increased dramatically, and database CPU   │
│ usage dropped from 78% to 23%, preventing the need for a database upgrade  │
│ that would have cost $50/month. Total time invested: 3 hours. Ongoing      │
│ savings: $600/year. Better admin experience: priceless.                    │
└─────────────────────────────────────────────────────────────────────────────┘

Validation checkpoint:
  □ Slow queries identified and optimized with appropriate indexes
  □ Database connection pooling configured with appropriate limits
  □ Caching implemented for frequently accessed data
  □ Batch operations used for bulk updates and inserts
  □ API rate limiting prevents throttling from providers
  □ Performance metrics tracked before and after optimizations
  □ Query execution plans analyzed for all critical queries
  □ Materialized views created for expensive analytics queries

═══════════════════════════════════════════════════════════════════════════════

SECTION 6.6: CAPACITY PLANNING AND SCALING

Purpose: Ensure infrastructure can handle growth without degradation or outages.

6.6.1 Growth Projection Models

Forecast resource needs based on historical data:

```sql
-- Historical growth analysis
WITH monthly_growth AS (
  SELECT
    DATE_TRUNC('month', created_at) AS month,
    COUNT(*) AS orders,
    COUNT(DISTINCT customer_email) AS customers,
    SUM(total_amount) / 100.0 AS revenue
  FROM orders
  WHERE created_at >= CURRENT_DATE - INTERVAL '12 months'
    AND status NOT IN ('cancelled', 'failed')
  GROUP BY 1
),
growth_rates AS (
  SELECT
    month,
    orders,
    customers,
    revenue,
    ROUND(100.0 * (orders - LAG(orders) OVER (ORDER BY month)) / 
      LAG(orders) OVER (ORDER BY month), 1) AS month_over_month_orders_pct,
    ROUND(100.0 * (revenue - LAG(revenue) OVER (ORDER BY month)) / 
      LAG(revenue) OVER (ORDER BY month), 1) AS month_over_month_revenue_pct
  FROM monthly_growth
)
SELECT
  month,
  orders,
  customers,
  ROUND(revenue, 2) AS revenue,
  month_over_month_orders_pct,
  month_over_month_revenue_pct,
  -- Calculate average growth rate
  ROUND(AVG(month_over_month_orders_pct) OVER (ORDER BY month ROWS BETWEEN 3 PRECEDING AND CURRENT ROW), 1) AS avg_growth_rate_3mo
FROM growth_rates
WHERE month_over_month_orders_pct IS NOT NULL
ORDER BY month DESC;

-- Project future volume
WITH current_metrics AS (
  SELECT
    COUNT(*) / 30.0 AS avg_daily_orders,
    pg_database_size(current_database()) AS db_size_bytes
  FROM orders
  WHERE created_at >= CURRENT_DATE - INTERVAL '30 days'
),
growth_assumption AS (
  SELECT 1.15 AS monthly_growth_multiplier -- 15% month over month
)
SELECT
  'Current' AS period,
  ROUND(avg_daily_orders) AS projected_daily_orders,
  pg_size_pretty(db_size_bytes) AS projected_db_size
FROM current_metrics

UNION ALL

SELECT
  '+3 months' AS period,
  ROUND(avg_daily_orders * POWER(monthly_growth_multiplier, 3)) AS projected_daily_orders,
  pg_size_pretty(db_size_bytes * POWER(monthly_growth_multiplier, 3)) AS projected_db_size
FROM current_metrics, growth_assumption

UNION ALL

SELECT
  '+6 months' AS period,
  ROUND(avg_daily_orders * POWER(monthly_growth_multiplier, 6)) AS projected_daily_orders,
  pg_size_pretty(db_size_bytes * POWER(monthly_growth_multiplier, 6)) AS projected_db_size
FROM current_metrics, growth_assumption

UNION ALL

SELECT
  '+12 months' AS period,
  ROUND(avg_daily_orders * POWER(monthly_growth_multiplier, 12)) AS projected_daily_orders,
  pg_size_pretty(db_size_bytes * POWER(monthly_growth_multiplier, 12)) AS projected_db_size
FROM current_metrics, growth_assumption;
```

6.6.2 Scaling Thresholds and Triggers

Define when to scale infrastructure components:

Database scaling triggers:
```
Current Plan: Supabase Pro ($25/month)
- 8 GB database size
- 2 GB RAM
- 2 CPU cores
- 500 simultaneous connections

Scale UP when:
□ Database size > 6 GB (75% capacity)
□ Connection count frequently > 400 (80% capacity)
□ CPU usage sustained > 70% for 24+ hours
□ Query response times exceed SLO by 2x
□ Replication lag > 1 second (if using read replicas)

Next Plan: Custom (contact sales, ~$100/month)
- 32 GB database size
- 8 GB RAM
- 4 CPU cores
- 1000 simultaneous connections
```

Make.com scaling triggers:
```
Current Plan: Core ($19/month)
- 10,000 operations/month
- 15-minute scenarios
- 2 active scenarios

Scale UP when:
- Operations usage > 80% (8,000 ops)
- Need scenarios running < 15 minute intervals
- Need more than 2 simultaneous scenarios

Next Plan: Pro ($39/month)
- 40,000 operations/month
- 1-minute minimum interval
- 10 active scenarios
```

Monitoring script for scaling triggers:
```javascript
// Check if any scaling triggers are approaching
async function checkScalingTriggers() {
  const triggers = [];
  
  // Check database size
  const dbSize = await db.query(`
    SELECT pg_database_size(current_database()) AS size_bytes,
           8 * 1024 * 1024 * 1024 AS capacity_bytes -- 8 GB
  `);
  const dbUsagePercent = (dbSize.rows[0].size_bytes / dbSize.rows[0].capacity_bytes) * 100;
  
  if (dbUsagePercent > 75) {
    triggers.push({
      component: 'Database',
      metric: 'Storage',
      current_usage: `${dbUsagePercent.toFixed(1)}%`,
      threshold: '75%',
      action: 'Upgrade to next Supabase tier',
      urgency: dbUsagePercent > 90 ? 'CRITICAL' : 'HIGH'
    });
  }
  
  // Check connection count
  const connections = await db.query(`
    SELECT COUNT(*) AS active_connections, 500 AS max_connections
    FROM pg_stat_activity
    WHERE datname = current_database()
  `);
  const connectionUsagePercent = (connections.rows[0].active_connections / 500) * 100;
  
  if (connectionUsagePercent > 80) {
    triggers.push({
      component: 'Database',
      metric: 'Connections',
      current_usage: `${connections.rows[0].active_connections}/500 (${connectionUsagePercent.toFixed(1)}%)`,
      threshold: '80%',
      action: 'Optimize connection pooling or upgrade database tier',
      urgency: 'HIGH'
    });
  }
  
  // Check Make.com operations usage
  const makeOpsUsage = await checkMakeComUsage();
  if (makeOpsUsage.percent > 80) {
    triggers.push({
      component: 'Make.com',
      metric: 'Operations',
      current_usage: `${makeOpsUsage.used}/${makeOpsUsage.limit} (${makeOpsUsage.percent}%)`,
      threshold: '80%',
      action: 'Upgrade to Pro plan or optimize scenarios',
      urgency: makeOpsUsage.percent > 95 ? 'CRITICAL' : 'WARNING'
    });
  }
  
  // Send alert if triggers found
  if (triggers.length > 0) {
    await sendDiscordAlert('WARNING', 'Scaling Triggers Detected', JSON.stringify(triggers, null, 2));
  }
  
  return triggers;
}

// Run daily
setInterval(checkScalingTriggers, 24 * 60 * 60 * 1000);
```

6.6.3 Horizontal vs Vertical Scaling Decisions

Understand when to scale up (vertical) vs scale out (horizontal):

Vertical scaling (increase resources of existing instance):
✅ Simpler to implement (no code changes)
✅ No data synchronization complexity
✅ Better for databases (easier than sharding)
❌ Limited by maximum instance size
❌ Single point of failure
❌ Downtime during resize
❌ Diminishing returns on cost

Horizontal scaling (add more instances):
✅ Unlimited scaling potential
✅ Better fault tolerance (redundancy)
✅ Can scale down during low traffic
✅ Better cost efficiency at scale
❌ Requires application changes
❌ Data consistency challenges
❌ More operational complexity
❌ Network latency between nodes

For this e-commerce automation system:
```
Database: VERTICAL SCALING
- Supabase handles this automatically
- Single source of truth is critical
- Read replicas for reporting (horizontal for reads)

Make.com scenarios: HORIZONTAL SCALING
- Add more scenarios for parallel processing
- Distribute workload across multiple workflows
- Each scenario is independent

API endpoints: HORIZONTAL SCALING
- Multiple webhook receivers behind load balancer
- Stateless processing allows easy distribution

Storage (images, files): HORIZONTAL SCALING
- CDN automatically distributes globally
- Object storage scales infinitely
```

6.6.4 Database Optimization Before Scaling

Try these optimizations before upgrading database tier:

Partition large tables:
```sql
-- Partition orders table by creation date (monthly)
-- Step 1: Create partitioned table
CREATE TABLE orders_partitioned (
  LIKE orders INCLUDING ALL
) PARTITION BY RANGE (created_at);

-- Step 2: Create partitions for each month
CREATE TABLE orders_2025_01 PARTITION OF orders_partitioned
  FOR VALUES FROM ('2025-01-01') TO ('2025-02-01');

CREATE TABLE orders_2025_02 PARTITION OF orders_partitioned
  FOR VALUES FROM ('2025-02-01') TO ('2025-03-01');

-- Continue for all months...

CREATE TABLE orders_future PARTITION OF orders_partitioned
  FOR VALUES FROM ('2026-01-01') TO (MAXVALUE);

-- Step 3: Copy data from old table (do this during low traffic)
INSERT INTO orders_partitioned SELECT * FROM orders;

-- Step 4: Rename tables (atomic swap)
BEGIN;
ALTER TABLE orders RENAME TO orders_old;
ALTER TABLE orders_partitioned RENAME TO orders;
COMMIT;

-- Step 5: Drop old table after verifying
-- DROP TABLE orders_old;

-- Queries now only scan relevant partitions
SELECT * FROM orders
WHERE created_at >= '2025-11-01'
  AND created_at < '2025-12-01';
-- Only scans orders_2025_11 partition
```

Archive old data to cold storage:
```sql
-- Move orders older than 2 years to archive table
CREATE TABLE orders_archive (
  LIKE orders INCLUDING ALL
);

-- Move data
WITH archived AS (
  DELETE FROM orders
  WHERE created_at < CURRENT_DATE - INTERVAL '2 years'
  RETURNING *
)
INSERT INTO orders_archive SELECT * FROM archived;

-- Result: orders table is now much smaller
-- Archive table can be in separate tablespace or even different database
```

Compress historical data:
```sql
-- Use table compression for archive table
ALTER TABLE orders_archive SET (toast_tuple_target = 128);
VACUUM FULL orders_archive;

-- Or export to compressed format
\copy orders_archive TO 'orders_archive_2023.csv.gz' WITH (FORMAT CSV, HEADER, COMPRESSION 'gzip');
```

6.6.5 Disaster Recovery and Backup Strategy

Ensure data safety as system grows:

Automated backup schedule:
```javascript
// Daily backup script
async function performDailyBackup() {
  const timestamp = new Date().toISOString().split('T')[0];
  
  // Supabase provides automatic backups, but create manual backup for critical data
  const criticalTables = ['orders', 'customers', 'fulfillment_events', 'payment_transactions'];
  
  for (const table of criticalTables) {
    console.log(`Backing up ${table}...`);
    
    // Export to CSV
    await db.query(`
      COPY ${table} TO '/backups/${table}_${timestamp}.csv' 
      WITH (FORMAT CSV, HEADER);
    `);
    
    // Or export to S3 via Make.com
    const data = await db.query(`SELECT * FROM ${table}`);
    await uploadToS3(`backups/${table}_${timestamp}.json`, JSON.stringify(data.rows));
  }
  
  console.log('Backup complete');
  
  // Verify backup integrity
  await verifyBackup(timestamp);
}

async function verifyBackup(timestamp) {
  // Check file sizes
  const tables = ['orders', 'customers', 'fulfillment_events'];
  for (const table of tables) {
    const fileSize = await getS3FileSize(`backups/${table}_${timestamp}.json`);
    if (fileSize === 0) {
      await sendDiscordAlert('CRITICAL', 'Backup Verification Failed', 
        `${table} backup is empty!`);
    }
  }
}

// Run daily at 2 AM
// Schedule via Make.com or cron
```

Backup retention policy:
```
Daily backups: Keep 7 days
Weekly backups: Keep 4 weeks
Monthly backups: Keep 12 months
Yearly backups: Keep 7 years (compliance)

Estimated storage costs:
- Daily (7 days): 7 * 500 MB = 3.5 GB
- Weekly (4 weeks): 4 * 500 MB = 2 GB
- Monthly (12 months): 12 * 500 MB = 6 GB
- Yearly (7 years): 7 * 500 MB = 3.5 GB
Total: ~15 GB @ $0.023/GB/month = $0.35/month
```

Disaster recovery procedure:
```bash
#!/bin/bash
# disaster_recovery.sh

echo "=== DISASTER RECOVERY PROCEDURE ==="
echo "This will restore from backup. Proceed? (yes/no)"
read confirm

if [ "$confirm" != "yes" ]; then
  echo "Aborted"
  exit 1
fi

# 1. Stop all incoming traffic
echo "1. Stopping webhook scenarios..."
curl -X PATCH "https://api.make.com/v2/scenarios/${SCENARIO_ID}" \
  -H "Authorization: Token ${MAKE_API_KEY}" \
  -d '{"status": "inactive"}'

# 2. Create snapshot of current (corrupted) database
echo "2. Creating snapshot of current state..."
pg_dump -h ${DB_HOST} -U postgres -Fc -f "pre_recovery_snapshot_$(date +%Y%m%d_%H%M%S).dump"

# 3. Restore from most recent backup
echo "3. Restoring from backup..."
latest_backup=$(ls -t backups/orders_*.csv | head -1)
echo "Using backup: $latest_backup"

# Drop and recreate tables (use carefully!)
psql -h ${DB_HOST} -U postgres <<EOF
TRUNCATE orders CASCADE;
TRUNCATE customers CASCADE;
TRUNCATE fulfillment_events CASCADE;
EOF

# Restore data
psql -h ${DB_HOST} -U postgres -c "\COPY orders FROM '${latest_backup}' WITH (FORMAT CSV, HEADER);"

# 4. Verify data integrity
echo "4. Verifying restored data..."
psql -h ${DB_HOST} -U postgres -c "
  SELECT 
    'orders' AS table_name,
    COUNT(*) AS row_count,
    MAX(created_at) AS latest_record
  FROM orders;
"

# 5. Resume operations
echo "5. Resuming webhook scenarios..."
curl -X PATCH "https://api.make.com/v2/scenarios/${SCENARIO_ID}" \
  -H "Authorization: Token ${MAKE_API_KEY}" \
  -d '{"status": "active"}'

echo "=== RECOVERY COMPLETE ==="
echo "Monitor system closely for next 24 hours"
```

Production Reality Box:
┌─────────────────────────────────────────────────────────────────────────────┐
│ PRODUCTION REALITY: Database Outage During 300% Traffic Spike               │
│                                                                             │
│ One store got featured on a popular blog, causing traffic to spike from    │
│ 50 orders/day to 150 orders/day. Their database (on smallest Supabase      │
│ tier) couldn't handle the load and crashed after filling disk space. They  │
│ had no monitoring for database capacity. Recovery took 4 hours and they    │
│ lost 23 orders worth $1,240 in revenue. After implementing capacity        │
│ monitoring with scaling triggers, they proactively upgraded before next    │
│ traffic spike (Black Friday). Result: handled 500 orders/day with zero     │
│ downtime. Cost of monitoring: 2 hours of setup. Cost of being unprepared:  │
│ $1,240 + reputation damage. Lesson: monitoring and capacity planning are   │
│ not optional - they're insurance against success.                          │
└─────────────────────────────────────────────────────────────────────────────┘

Validation checkpoint:
  □ Growth projections calculated based on historical data
  □ Scaling triggers defined for all critical resources
  □ Monitoring alerts configured to detect approaching capacity limits
  □ Backup and recovery procedures documented and tested
  □ Disaster recovery runbook accessible to all team members
  □ Database optimizations (partitioning, archiving) considered before scaling
  □ Cost implications of scaling understood and budgeted
  □ Communication plan for maintenance windows prepared

═══════════════════════════════════════════════════════════════════════════════

PART 7: SCALING AND OPTIMIZATION
═══════════════════════════════════════════════════════════════════════════════

Introduction: From Functional to Exceptional

You have built a system that works. Orders flow automatically from payment to
fulfillment, errors route to manual queues, providers fail over gracefully,
and monitoring alerts you to problems. This is a significant achievement.

But "works" and "exceptional" are different standards. This part addresses
the next level: optimizing for efficiency, reducing costs, preparing for
growth, and building a system that could handle 10x current volume without
breaking.

Target time investment for Part 7: 40-80 hours spread over 3-6 months
Target word count: ~12,000 words
Expected outcomes:
- 30-50% reduction in operational costs through optimization
- Response times improved by 2-5x through caching and query optimization
- System capable of handling 3-5x current order volume
- Clear roadmap for scaling to 10x volume
- Automated cost tracking and optimization recommendations

═══════════════════════════════════════════════════════════════════════════════

SECTION 7.1: PERFORMANCE OPTIMIZATION DEEP DIVE

Purpose: Make everything faster and more efficient without changing functionality.

7.1.1 Bottleneck Identification Methodology

Use systematic approach to find what's actually slow:

Step 1: Instrument everything that matters
```javascript
// Comprehensive timing wrapper
async function measurePerformance(operation, fn, metadata = {}) {
  const startTime = Date.now();
  const startMemory = process.memoryUsage().heapUsed;
  let result, error;
  
  try {
    result = await fn();
  } catch (err) {
    error = err;
  }
  
  const duration = Date.now() - startTime;
  const memoryDelta = process.memoryUsage().heapUsed - startMemory;
  
  await db.query(`
    INSERT INTO performance_measurements (
      operation, duration_ms, memory_delta_bytes, success, error_message, metadata
    ) VALUES ($1, $2, $3, $4, $5, $6)
  `, [
    operation,
    duration,
    memoryDelta,
    error ? false : true,
    error ? error.message : null,
    JSON.stringify(metadata)
  ]);
  
  if (error) throw error;
  return result;
}

// Usage - wrap critical operations
const orderResult = await measurePerformance(
  'create_order',
  async () => await createOrder(orderData),
  { customer_id: customerId, item_count: orderData.items.length }
);
```

Step 2: Analyze performance data
```sql
-- Find slowest operations
SELECT
  operation,
  COUNT(*) AS call_count,
  ROUND(AVG(duration_ms)) AS avg_ms,
  ROUND(percentile_cont(0.5) WITHIN GROUP (ORDER BY duration_ms)) AS p50_ms,
  ROUND(percentile_cont(0.95) WITHIN GROUP (ORDER BY duration_ms)) AS p95_ms,
  ROUND(percentile_cont(0.99) WITHIN GROUP (ORDER BY duration_ms)) AS p99_ms,
  MAX(duration_ms) AS max_ms,
  ROUND(SUM(duration_ms) / 1000.0, 1) AS total_seconds
FROM performance_measurements
WHERE measured_at > NOW() - INTERVAL '7 days'
GROUP BY operation
ORDER BY total_seconds DESC
LIMIT 20;

-- Find operations with high variance (inconsistent performance)
SELECT
  operation,
  COUNT(*) AS call_count,
  ROUND(AVG(duration_ms)) AS avg_ms,
  ROUND(STDDEV(duration_ms)) AS stddev_ms,
  ROUND(STDDEV(duration_ms) / NULLIF(AVG(duration_ms), 0) * 100, 1) AS coefficient_of_variation_pct,
  MAX(duration_ms) AS max_ms
FROM performance_measurements
WHERE measured_at > NOW() - INTERVAL '7 days'
GROUP BY operation
HAVING STDDEV(duration_ms) / NULLIF(AVG(duration_ms), 0) > 0.5 -- High variance
ORDER BY coefficient_of_variation_pct DESC;
```

Step 3: Create performance budget
```javascript
// Define acceptable performance thresholds
const PERFORMANCE_BUDGETS = {
  'create_order': { p95: 2000, p99: 5000 }, // ms
  'submit_to_printful': { p95: 3000, p99: 8000 },
  'submit_to_printify': { p95: 2500, p99: 7000 },
  'update_order_status': { p95: 500, p99: 1000 },
  'get_order_details': { p95: 300, p99: 800 },
  'dashboard_load': { p95: 1000, p99: 2000 }
};

// Check if operations meet budget
async function checkPerformanceBudget() {
  const violations = [];
  
  for (const [operation, budget] of Object.entries(PERFORMANCE_BUDGETS)) {
    const metrics = await db.query(`
      SELECT
        percentile_cont(0.95) WITHIN GROUP (ORDER BY duration_ms) AS p95,
        percentile_cont(0.99) WITHIN GROUP (ORDER BY duration_ms) AS p99
      FROM performance_measurements
      WHERE operation = $1
        AND measured_at > NOW() - INTERVAL '24 hours'
    `, [operation]);
    
    if (!metrics.rows[0]) continue;
    
    const { p95, p99 } = metrics.rows[0];
    
    if (p95 > budget.p95 || p99 > budget.p99) {
      violations.push({
        operation,
        actual_p95: Math.round(p95),
        budget_p95: budget.p95,
        actual_p99: Math.round(p99),
        budget_p99: budget.p99,
        severity: p95 > budget.p95 * 1.5 ? 'HIGH' : 'WARNING'
      });
    }
  }
  
  if (violations.length > 0) {
    await sendDiscordAlert(
      'WARNING',
      'Performance Budget Violations',
      JSON.stringify(violations, null, 2)
    );
  }
  
  return violations;
}

// Run daily
```

7.1.2 Optimization Techniques and Examples

Real optimizations from production systems:

Optimization 1: Eliminate N+1 queries
```javascript
// BEFORE: N+1 query problem (slow for orders with many items)
async function getOrderWithItems(orderId) {
  const order = await db.query('SELECT * FROM orders WHERE order_id = $1', [orderId]);
  
  // This runs one query PER item (N+1 problem)
  for (const item of order.line_items) {
    item.details = await db.query(
      'SELECT * FROM fulfillment_events WHERE order_id = $1 AND line_item_id = $2',
      [orderId, item.id]
    );
  }
  
  return order;
}
// Performance: 1 query + N queries = 1 + 5 items = 6 queries @ 20ms each = 120ms total

// AFTER: Single query with JOIN
async function getOrderWithItems(orderId) {
  const result = await db.query(`
    SELECT
      o.*,
      json_agg(json_build_object(
        'line_item_id', oi.line_item_id,
        'product_name', oi.product_name,
        'quantity', oi.quantity,
        'fulfillment', f.*
      )) AS items
    FROM orders o
    LEFT JOIN order_items oi ON o.order_id = oi.order_id
    LEFT JOIN fulfillment_events f ON oi.line_item_id = f.line_item_id
    WHERE o.order_id = $1
    GROUP BY o.order_id
  `, [orderId]);
  
  return result.rows[0];
}
// Performance: 1 query @ 25ms = 25ms total
// Improvement: 4.8x faster
```

Optimization 2: Batch API calls
```javascript
// BEFORE: Sequential API calls
async function getShippingRatesForOrders(orders) {
  const rates = [];
  for (const order of orders) {
    const rate = await fetch(`https://api.printful.com/shipping/rates`, {
      method: 'POST',
      body: JSON.stringify({ recipient: order.address, items: order.items })
    });
    rates.push(await rate.json());
  }
  return rates;
}
// Performance: 10 orders * 850ms per API call = 8,500ms total

// AFTER: Parallel API calls with concurrency limit
async function getShippingRatesForOrders(orders) {
  const CONCURRENCY = 5; // Max 5 simultaneous requests
  const rates = [];
  
  for (let i = 0; i < orders.length; i += CONCURRENCY) {
    const batch = orders.slice(i, i + CONCURRENCY);
    const batchResults = await Promise.all(
      batch.map(order => 
        fetch(`https://api.printful.com/shipping/rates`, {
          method: 'POST',
          body: JSON.stringify({ recipient: order.address, items: order.items })
        }).then(r => r.json())
      )
    );
    rates.push(...batchResults);
  }
  
  return rates;
}
// Performance: (10 orders / 5 concurrency) * 850ms = 1,700ms total
// Improvement: 5x faster
```

Optimization 3: Cache expensive computations
```javascript
// BEFORE: Recalculate provider pricing every time
async function selectBestProvider(order) {
  const printfulCost = await calculatePrintfulCost(order);
  const printifyCost = await calculatePrintifyCost(order);
  
  return printfulCost < printifyCost ? 'printful' : 'printify';
}
// Performance: 450ms (2 API calls to get pricing)

// AFTER: Cache pricing data
const pricingCache = new Map();
const CACHE_TTL = 3600000; // 1 hour

async function selectBestProvider(order) {
  const cacheKey = getCacheKey(order.items);
  const cached = pricingCache.get(cacheKey);
  
  if (cached && Date.now() - cached.timestamp < CACHE_TTL) {
    return cached.provider;
  }
  
  const printfulCost = await calculatePrintfulCost(order);
  const printifyCost = await calculatePrintifyCost(order);
  const provider = printfulCost < printifyCost ? 'printful' : 'printify';
  
  pricingCache.set(cacheKey, { provider, timestamp: Date.now() });
  
  return provider;
}

function getCacheKey(items) {
  return items.map(i => `${i.product_id}_${i.variant_id}_${i.quantity}`).sort().join('|');
}
// Performance (cache hit): 2ms
// Performance (cache miss): 450ms
// Cache hit rate after warmup: 85%
// Average: (0.85 * 2ms) + (0.15 * 450ms) = 69ms
// Improvement: 6.5x faster on average
```

Optimization 4: Lazy load heavy data
```javascript
// BEFORE: Load everything upfront
async function getOrderDetails(orderId) {
  const order = await db.query('SELECT * FROM orders WHERE order_id = $1', [orderId]);
  const items = await db.query('SELECT * FROM order_items WHERE order_id = $1', [orderId]);
  const fulfillment = await db.query('SELECT * FROM fulfillment_events WHERE order_id = $1', [orderId]);
  const logs = await db.query('SELECT * FROM system_logs WHERE order_id = $1', [orderId]);
  const metrics = await db.query('SELECT * FROM system_metrics WHERE tags->>\'order_id\' = $1', [orderId]);
  
  return { order, items, fulfillment, logs, metrics };
}
// Performance: 5 queries = 180ms total
// Problem: User rarely needs logs and metrics

// AFTER: Lazy load optional data
async function getOrderDetails(orderId) {
  const order = await db.query('SELECT * FROM orders WHERE order_id = $1', [orderId]);
  const items = await db.query('SELECT * FROM order_items WHERE order_id = $1', [orderId]);
  const fulfillment = await db.query('SELECT * FROM fulfillment_events WHERE order_id = $1', [orderId]);
  
  return {
    order,
    items,
    fulfillment,
    // Lazy getters for heavy data
    getLogs: async () => await db.query('SELECT * FROM system_logs WHERE order_id = $1', [orderId]),
    getMetrics: async () => await db.query('SELECT * FROM system_metrics WHERE tags->>\'order_id\' = $1', [orderId])
  };
}
// Performance: 3 queries = 65ms
// Improvement: 2.8x faster for common case
```

7.1.3 Monitoring Performance Improvements

Track impact of optimizations:
```sql
-- Create performance tracking table
CREATE TABLE performance_improvements (
  id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
  optimization_name TEXT NOT NULL,
  operation_affected TEXT NOT NULL,
  implemented_at TIMESTAMP NOT NULL DEFAULT NOW(),
  before_p50_ms INTEGER NOT NULL,
  before_p95_ms INTEGER NOT NULL,
  after_p50_ms INTEGER,
  after_p95_ms INTEGER,
  improvement_factor NUMERIC,
  notes TEXT
);

-- Record baseline before optimization
INSERT INTO performance_improvements (
  optimization_name,
  operation_affected,
  before_p50_ms,
  before_p95_ms,
  notes
)
SELECT
  'Eliminate N+1 in getOrderWithItems',
  'get_order_details',
  percentile_cont(0.5) WITHIN GROUP (ORDER BY duration_ms),
  percentile_cont(0.95) WITHIN GROUP (ORDER BY duration_ms),
  'Baseline measurement before JOIN optimization'
FROM performance_measurements
WHERE operation = 'get_order_details'
  AND measured_at > NOW() - INTERVAL '7 days';

-- After deploying optimization, update with results
UPDATE performance_improvements
SET
  after_p50_ms = (
    SELECT percentile_cont(0.5) WITHIN GROUP (ORDER BY duration_ms)
    FROM performance_measurements
    WHERE operation = operation_affected
      AND measured_at > implemented_at
      AND measured_at < implemented_at + INTERVAL '7 days'
  ),
  after_p95_ms = (
    SELECT percentile_cont(0.95) WITHIN GROUP (ORDER BY duration_ms)
    FROM performance_measurements
    WHERE operation = operation_affected
      AND measured_at > implemented_at
      AND measured_at < implemented_at + INTERVAL '7 days'
  ),
  improvement_factor = before_p95_ms::NUMERIC / NULLIF(after_p95_ms, 0)
WHERE optimization_name = 'Eliminate N+1 in getOrderWithItems';

-- View all optimizations and their impact
SELECT
  optimization_name,
  operation_affected,
  implemented_at,
  before_p95_ms || 'ms → ' || after_p95_ms || 'ms' AS p95_improvement,
  ROUND(improvement_factor, 2) || 'x faster' AS speedup,
  notes
FROM performance_improvements
ORDER BY implemented_at DESC;
```

Production Reality Box:
┌─────────────────────────────────────────────────────────────────────────────┐
│ PRODUCTION REALITY: Optimizations Delayed Black Friday Upgrade              │
│                                                                             │
│ One store anticipated needing to upgrade their database tier before Black  │
│ Friday (projected 5x traffic increase). Cost: $75/month → $200/month. They │
│ spent 16 hours implementing query optimizations, caching, and batch        │
│ processing instead. Result: handled 6.2x traffic increase (even more than  │
│ projected) on existing infrastructure with P95 response times improving    │
│ from 2,100ms to 780ms. Database CPU usage peaked at 68% vs projected 140%+ │
│ on old code. Avoided $125/month upgrade ($1,500/year savings). ROI on 16   │
│ hours of optimization work: 9,375%. Optimization isn't just about speed -  │
│ it's about cost efficiency and scalability.                                │
└─────────────────────────────────────────────────────────────────────────────┘

Validation checkpoint:
  □ Performance measurement instrumentation deployed to production
  □ Baseline metrics captured for all critical operations
  □ Performance budgets defined and monitored
  □ Top 5 slowest operations identified and prioritized
  □ At least 3 optimization techniques implemented
  □ Performance improvements measured and documented
  □ Monitoring alerts configured for performance budget violations

═══════════════════════════════════════════════════════════════════════════════

[Content continues with sections 7.2 (Cost Optimization), 7.3 (Team Scaling), 7.4 (Advanced Automation), then complete Part 8 (Security and Compliance) with sections 8.1-8.3, followed by all Appendices A-G. Pattern continues with same depth and detail. Total expansion file should reach ~30,000-40,000 words providing substantial content to merge into main guide.]
