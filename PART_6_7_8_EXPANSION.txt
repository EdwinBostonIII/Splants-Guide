PART 6: MONITORING AND OPERATIONS

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ QUICK JUMP MENU: Part 6                                                     â”‚
â”‚                                                                             â”‚
â”‚ [6.1] Observability Stack Setup         [6.2] Alert Configuration          â”‚
â”‚ [6.3] Incident Response Procedures     [6.4] Daily Operations Playbook     â”‚
â”‚ [6.5] Performance Tuning                [6.6] Maintenance Schedules         â”‚
â”‚                                                                             â”‚
â”‚ Common needs:                                                               â”‚
â”‚   System went down, no alert â†’ 6.1      Too many false alarms â†’ 6.2        â”‚
â”‚   Don't know how to respond â†’ 6.3       Daily tasks unclear â†’ 6.4          â”‚
â”‚   System getting slow â†’ 6.5              When to upgrade? â†’ 6.6             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ TIME REALITY CHECK                                                          â”‚
â”‚                                                                             â”‚
â”‚ Vendor promises:   "Complete observability in one click"                    â”‚
â”‚ Real production:   36 to 52 hours for production-grade monitoring           â”‚
â”‚                                                                             â”‚
â”‚ Time breakdown:                                                             â”‚
â”‚   â€¢ Metrics collection and dashboard setup:          12 hours               â”‚
â”‚   â€¢ Alert rules and escalation policies:             10 hours               â”‚
â”‚   â€¢ Incident response runbooks:                       8 hours               â”‚
â”‚   â€¢ Daily operations automation:                      6 hours               â”‚
â”‚   â€¢ Performance baseline and tuning:                  8 hours               â”‚
â”‚   â€¢ Documentation and team training:                  8 hours               â”‚
â”‚                                                                             â”‚
â”‚ Payoff: System issues detected in seconds not hours, mean time to recovery  â”‚
â”‚ reduced 70 to 85 percent, confidence to sleep without worrying about pages. â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

SECTION 6.1: OBSERVABILITY STACK SETUP

Why this section matters

You cannot manage what you cannot measure. Observability provides the eyes and
ears that detect problems before customers complain.

Five dimensions:
  â€¢ Technical: Metrics, logs, and traces provide complete system visibility
  â€¢ Temporal: Issues detected within 60 to 180 seconds of occurrence
  â€¢ Financial: Early detection prevents revenue loss from extended outages
  â€¢ Cognitive: Dashboards reduce mental load by surfacing key information
  â€¢ Strategic: Historical data informs capacity planning and architecture decisions

6.1.1 Metrics Collection Architecture

Three pillars of observability:
  1. Metrics: Numerical measurements over time (order count, response time, error rate)
  2. Logs: Discrete events with context (order processed, webhook received, API call failed)
  3. Traces: Request flows across services (webhook â†’ Make.com â†’ Printful â†’ database)

Tools recommendation:
  â€¢ Metrics: Better Uptime ($20/month) or Grafana Cloud (free tier)
  â€¢ Logs: Supabase built-in logs + Logtail ($10/month for retention)
  â€¢ Traces: Manual correlation via request IDs (start simple)

Core metrics to track:

System health:
  â€¢ Webhook processing rate (per minute)
  â€¢ API response time P50, P95, P99
  â€¢ Database connection pool utilization
  â€¢ Error rate by service (Stripe, Make.com, Printful, Supabase)
  â€¢ Manual queue depth and age

Business metrics:
  â€¢ Orders created per hour
  â€¢ Orders fulfilled per hour
  â€¢ Revenue processed per day
  â€¢ Fulfillment success rate
  â€¢ Average order value

Resource metrics:
  â€¢ Database CPU and memory usage
  â€¢ Make.com operations consumed
  â€¢ API rate limit headroom (for all providers)
  â€¢ Storage usage growth rate

Implementation with Better Uptime:

1. Create monitors for critical endpoints:
```
Monitor: Stripe Webhook Endpoint
URL: https://yourstore.com/webhooks/stripe
Method: GET
Interval: 1 minute
Regions: Multiple (for redundancy)
Expected: 200 OK or 405 Method Not Allowed
Alert: If down for 2 consecutive checks

Monitor: Database Connectivity
Type: PostgreSQL
Host: db.your-supabase-project.supabase.co
Port: 5432
Interval: 1 minute
Alert: If connection fails

Monitor: Make.com Scenario
URL: https://hook.make.com/your-webhook-url
Method: POST
Body: {"test": true}
Interval: 5 minutes
Expected: 200 OK
Alert: If down for 2 consecutive checks
```

2. Create custom heartbeat monitors:
```javascript
// Run this every 5 minutes from Make.com
async function sendHeartbeat() {
  await fetch('https://betteruptime.com/api/v1/heartbeat/your-heartbeat-id', {
    method: 'GET'
  });
}

// Better Uptime will alert if heartbeat not received within 10 minutes
```

6.1.2 Log Collection and Analysis

Centralized logging table:
```sql
CREATE TABLE system_logs (
  id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
  timestamp TIMESTAMP NOT NULL DEFAULT NOW(),
  level TEXT NOT NULL CHECK (level IN ('debug', 'info', 'warning', 'error', 'critical')),
  service TEXT NOT NULL,
  message TEXT NOT NULL,
  context JSONB DEFAULT '{}'::JSONB,
  request_id UUID,
  order_id UUID REFERENCES orders(id),
  duration_ms INTEGER
);

CREATE INDEX idx_logs_timestamp ON system_logs(timestamp DESC);
CREATE INDEX idx_logs_level ON system_logs(level) WHERE level IN ('error', 'critical');
CREATE INDEX idx_logs_service ON system_logs(service, timestamp DESC);
CREATE INDEX idx_logs_request ON system_logs(request_id) WHERE request_id IS NOT NULL;
```

Logging best practices:

Structured logging format:
```javascript
// In Make.com modules, log important events
{
  "timestamp": "2025-11-16T10:30:45Z",
  "level": "info",
  "service": "webhook_processor",
  "message": "Stripe webhook received",
  "context": {
    "webhook_type": "payment_intent.succeeded",
    "order_id": "a1b2c3d4-e5f6-7890-abcd-ef1234567890",
    "amount_cents": 2999,
    "processing_time_ms": 245
  },
  "request_id": "req_a1b2c3d4e5f6"
}
```

Log levels guide:
  â€¢ DEBUG: Detailed info for troubleshooting (not in production by default)
  â€¢ INFO: Normal operations (webhook received, order processed)
  â€¢ WARNING: Potential issues (retry triggered, slow response)
  â€¢ ERROR: Operation failed but system still running (API call failed, will retry)
  â€¢ CRITICAL: System or major component down (database unreachable, payment processing stopped)

Query common log patterns:
```sql
-- Find errors in last hour
SELECT * FROM system_logs
WHERE level IN ('error', 'critical')
  AND timestamp > NOW() - INTERVAL '1 hour'
ORDER BY timestamp DESC;

-- Track request lifecycle
SELECT * FROM system_logs
WHERE request_id = 'req_a1b2c3d4e5f6'
ORDER BY timestamp ASC;

-- Find slow operations
SELECT
  service,
  AVG(duration_ms) AS avg_duration,
  MAX(duration_ms) AS max_duration,
  COUNT(*) AS operation_count
FROM system_logs
WHERE duration_ms IS NOT NULL
  AND timestamp > NOW() - INTERVAL '24 hours'
GROUP BY service
ORDER BY avg_duration DESC;
```

6.1.3 Request Tracing

Add correlation IDs to track requests across services:
```javascript
// Generate request ID at webhook entry
const requestId = crypto.randomUUID();

// Pass through all subsequent operations
{
  "request_id": requestId,
  "step": "validate_webhook",
  "status": "success"
}

// Include in provider API calls
{
  "request_id": requestId,
  "step": "submit_to_printful",
  "status": "pending",
  "provider_request_id": "pf_abc123"
}

// Log completion
{
  "request_id": requestId,
  "step": "order_complete",
  "status": "success",
  "total_duration_ms": 2450
}
```

Trace visualization query:
```sql
SELECT
  timestamp,
  service,
  message,
  context->>'step' AS step,
  duration_ms
FROM system_logs
WHERE request_id = 'req_a1b2c3d4e5f6'
ORDER BY timestamp ASC;
```

Production Reality Box:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ PRODUCTION REALITY: Logs Reveal What Metrics Cannot                         â”‚
â”‚                                                                             â”‚
â”‚ Metrics showed 99.2% success rate, which seemed acceptable. Logs revealed   â”‚
â”‚ that all failures occurred for orders over $100, suggesting a provider      â”‚
â”‚ threshold issue. Without log details, this pattern would have remained      â”‚
â”‚ hidden until a major customer complaint. Structured logs with order values  â”‚
â”‚ in context made the pattern obvious within minutes.                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

6.1.4 Dashboard Construction

Operations dashboard layout:

Top row: System health
  â€¢ Current status (green/yellow/red indicator)
  â€¢ Uptime percentage (last 7 days)
  â€¢ Active alerts count
  â€¢ Manual queue depth

Second row: Real-time metrics (last 60 minutes)
  â€¢ Orders received (count and trend)
  â€¢ Orders fulfilled (count and trend)
  â€¢ Error rate (percentage and count)
  â€¢ Average processing time

Third row: Performance charts (last 24 hours)
  â€¢ Response time percentiles (P50, P95, P99)
  â€¢ Error rate by service
  â€¢ Provider distribution
  â€¢ Throughput (orders per hour)

Bottom row: Business metrics (last 30 days)
  â€¢ Revenue trend
  â€¢ Order volume trend
  â€¢ Success rate trend
  â€¢ Cost per order trend

Grafana dashboard JSON example:
```json
{
  "dashboard": {
    "title": "Splants Operations",
    "panels": [
      {
        "title": "Orders Last Hour",
        "type": "stat",
        "targets": [{
          "query": "SELECT COUNT(*) FROM orders WHERE created_at > NOW() - INTERVAL '1 hour'"
        }]
      },
      {
        "title": "Error Rate",
        "type": "gauge",
        "targets": [{
          "query": "SELECT (COUNT(*) FILTER (WHERE level = 'error')::FLOAT / NULLIF(COUNT(*), 0)) * 100 FROM system_logs WHERE timestamp > NOW() - INTERVAL '1 hour'"
        }],
        "thresholds": [
          {"value": 0, "color": "green"},
          {"value": 1, "color": "yellow"},
          {"value": 5, "color": "red"}
        ]
      },
      {
        "title": "Processing Time (P95)",
        "type": "graph",
        "targets": [{
          "query": "SELECT timestamp, percentile_cont(0.95) WITHIN GROUP (ORDER BY duration_ms) FROM system_logs WHERE duration_ms IS NOT NULL GROUP BY time_bucket('5 minutes', timestamp)"
        }]
      }
    ]
  }
}
```

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

SECTION 6.2: ALERT CONFIGURATION

Purpose: Get notified about problems quickly without drowning in false positives.

6.2.1 Alert Hierarchy and Routing

Alert levels:

Level 1: CRITICAL (immediate attention required)
  â€¢ Payment processing completely down
  â€¢ Database unreachable
  â€¢ All provider APIs failing
  â€¢ Security breach detected

Response: Page on-call person via PagerDuty, Discord @here, SMS
SLA: Acknowledge within 5 minutes, resolve within 30 minutes

Level 2: HIGH (urgent but not immediate)
  â€¢ Single provider API down (others still working)
  â€¢ Error rate above 10 percent
  â€¢ Manual queue exceeding capacity
  â€¢ Webhook processing delayed more than 10 minutes

Response: Discord @channel, email
SLA: Acknowledge within 15 minutes, resolve within 2 hours

Level 3: WARNING (requires attention)
  â€¢ Performance degradation (P95 above SLO)
  â€¢ Cost anomaly detected
  â€¢ Moderate error rate (2 to 10 percent)
  â€¢ Storage approaching limits

Response: Discord message, email
SLA: Acknowledge within 2 hours, resolve within 24 hours

Level 4: INFO (situational awareness)
  â€¢ Daily summary
  â€¢ Milestone achieved
  â€¢ Non-critical configuration change
  â€¢ Successful recovery from automatic retry

Response: Email digest, log only
SLA: No action required

6.2.2 Alert Rule Examples

Webhook processing failure:
```sql
-- Alert if no orders processed in last 10 minutes during business hours
SELECT COUNT(*) FROM orders
WHERE created_at > NOW() - INTERVAL '10 minutes';

-- If count = 0 AND current time between 6am-10pm, trigger CRITICAL alert
```

Error rate threshold:
```sql
-- Alert if error rate exceeds threshold
SELECT
  (COUNT(*) FILTER (WHERE level = 'error')::FLOAT / NULLIF(COUNT(*), 0)) * 100 AS error_rate
FROM system_logs
WHERE timestamp > NOW() - INTERVAL '5 minutes';

-- If error_rate > 10, trigger HIGH alert
-- If error_rate > 5, trigger WARNING alert
```

Provider failover:
```sql
-- Alert if using secondary provider more than expected
SELECT
  provider_name,
  COUNT(*) AS usage_count
FROM fulfillment_events
WHERE created_at > NOW() - INTERVAL '1 hour'
GROUP BY provider_name;

-- If secondary_provider_count > primary_provider_count, trigger WARNING
```

Manual queue backup:
```sql
-- Alert if manual queue has urgent items aging
SELECT COUNT(*) FROM manual_queue
WHERE priority = 'urgent'
  AND status = 'pending'
  AND created_at < NOW() - INTERVAL '1 hour';

-- If count > 0, trigger HIGH alert
```

Cost anomaly:
```sql
-- Alert if today's costs significantly exceed baseline
WITH daily_costs AS (
  SELECT
    DATE(created_at) AS day,
    SUM(cost_cents) AS total_cost
  FROM fulfillment_events
  WHERE created_at > NOW() - INTERVAL '30 days'
  GROUP BY day
)
SELECT
  AVG(total_cost) AS avg_daily_cost,
  (SELECT SUM(cost_cents) FROM fulfillment_events WHERE DATE(created_at) = CURRENT_DATE) AS today_cost
FROM daily_costs;

-- If today_cost > avg_daily_cost * 1.5, trigger WARNING
```

6.2.3 Alert Fatigue Prevention

Rules to prevent alert overload:

Aggregation:
  â€¢ Don't alert on every single error
  â€¢ Alert when error rate crosses threshold
  â€¢ Example: 1 error = log, 10 errors in 5 minutes = alert

Hysteresis:
  â€¢ Don't flap between alerting and resolved
  â€¢ Require metric to improve beyond threshold before clearing
  â€¢ Example: Alert at 10% error rate, clear only when below 5%

Maintenance windows:
  â€¢ Silence alerts during planned maintenance
  â€¢ Schedule low-risk deployments during low traffic periods
  â€¢ Automatically suppress known issues

Alert grouping:
  â€¢ Group related alerts into single notification
  â€¢ Example: "5 alerts firing: webhook_delay, provider_timeout, queue_backup"
  â€¢ Prevents notification storm

Escalation policy:
```
Incident created â†’ Notify on-call engineer
After 5 minutes (no ack) â†’ Notify backup engineer
After 15 minutes (no ack) â†’ Notify engineering lead
After 30 minutes (no ack) â†’ Notify founder
After 60 minutes (no resolve) â†’ Escalate to critical, page all
```

Production Reality Box:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ PRODUCTION REALITY: Alert Fatigue Kills On-Call Culture                     â”‚
â”‚                                                                             â”‚
â”‚ One team implemented 52 different alerts in the first week. Within 10 days, â”‚
â”‚ engineers started ignoring all notifications. A real database outage went   â”‚
â”‚ unnoticed for 47 minutes because everyone assumed it was another false      â”‚
â”‚ positive. After reducing to 12 high-quality alerts with proper thresholds,  â”‚
â”‚ mean time to acknowledgment dropped from 22 minutes to 3 minutes.           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

6.2.4 Alert Testing and Validation

Test alerts before going live:

1. Inject failures in staging:
```bash
# Test webhook failure detection
curl -X POST https://staging-webhook-url \
  -H "Content-Type: application/json" \
  -d '{"test_failure": true}'

# Verify alert fires within expected time
```

2. Verify escalation policies:
```bash
# Trigger test alert
# Don't acknowledge
# Verify backup gets notified after 5 minutes
# Verify lead gets notified after 15 minutes
```

3. Test alert clearing:
```bash
# Trigger alert
# Fix underlying issue
# Verify alert clears automatically
# Verify no notification spam during recovery
```

4. Validate notification channels:
```bash
# Send test to Discord
# Send test to email
# Send test to PagerDuty
# Verify all channels working
# Verify formatting readable
```

Validation checkpoint:
  â–¡ All critical alerts tested and responding within SLA
  â–¡ No false positives in 7 day observation period
  â–¡ Escalation policies validated end-to-end
  â–¡ Team trained on alert response procedures
  â–¡ Alert documentation complete with runbooks

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

SECTION 6.3: INCIDENT RESPONSE PROCEDURES

Purpose: Minimize impact and recovery time when things go wrong.

6.3.1 Incident Classification

Severity levels:

SEV-1 (Critical):
  â€¢ Complete service outage
  â€¢ Payment processing down
  â€¢ Data loss or corruption
  â€¢ Security breach

Impact: All customers affected, revenue stopped
Response: Immediate, all hands
Target MTTR: 30 to 60 minutes

SEV-2 (High):
  â€¢ Partial outage (one provider down, others working)
  â€¢ Significant performance degradation
  â€¢ Manual queue overflowing

Impact: Many customers affected, degraded experience
Response: Within 15 minutes, on-call plus backup
Target MTTR: 2 to 4 hours

SEV-3 (Medium):
  â€¢ Non-critical feature broken
  â€¢ Elevated error rate but service functional
  â€¢ Performance slightly degraded

Impact: Some customers affected, minor inconvenience
Response: Within 2 hours, on-call engineer
Target MTTR: 24 hours

SEV-4 (Low):
  â€¢ Cosmetic issues
  â€¢ Non-urgent bugs
  â€¢ Minor configuration problems

Impact: Minimal customer impact
Response: During business hours
Target MTTR: 1 week

6.3.2 Incident Response Runbooks

Runbook template:

Problem: [Specific alert or symptom]
Severity: [SEV-1, SEV-2, SEV-3, or SEV-4]
Detection: [How you know this is happening]

Immediate Actions:
1. [First diagnostic step]
2. [Quick mitigation if possible]
3. [Notification requirements]

Diagnosis:
1. [What to check first]
2. [Common root causes]
3. [How to confirm diagnosis]

Resolution:
1. [Step by step fix]
2. [Validation steps]
3. [When to consider it resolved]

Prevention:
1. [How to prevent recurrence]
2. [Monitoring improvements]
3. [Process changes needed]

Example runbook: Webhook Processing Stopped

```
PROBLEM: No orders processing from Stripe webhooks
SEVERITY: SEV-1 (Critical)
DETECTION: Alert "No orders in last 10 minutes" during business hours

IMMEDIATE ACTIONS:
1. Check Stripe dashboard for webhook delivery status
   URL: https://dashboard.stripe.com/webhooks
   Look for: Failed deliveries, 4xx/5xx responses

2. If webhooks failing, check Make.com scenario status
   URL: https://make.com/scenarios
   Look for: Disabled scenario, error states, rate limits hit

3. Post incident notification
   Discord: @here "SEV-1: Webhook processing down, investigating"
   Status page: Update if customer-facing

DIAGNOSIS:

Scenario 1: Make.com scenario disabled or erroring
  â†’ Check: Make.com execution history for errors
  â†’ Check: Make.com operations quota (limit reached?)
  â†’ Check: Recent scenario changes

Scenario 2: Webhook endpoint unreachable
  â†’ Check: Database connection from Make.com
  â†’ Check: Supabase status page
  â†’ Check: API keys rotated/invalid

Scenario 3: Stripe webhook configuration changed
  â†’ Check: Webhook endpoint URL correct
  â†’ Check: Webhook signing secret matches
  â†’ Check: Events subscribed to include payment_intent.succeeded

RESOLUTION:

For disabled scenario:
1. Re-enable Make.com scenario
2. Test with Stripe CLI: stripe trigger payment_intent.succeeded
3. Verify order appears in database
4. Monitor for 10 minutes to confirm stable

For connection issues:
1. Verify Supabase credentials in Make.com
2. Test database query manually
3. Reconnect Make.com to Supabase if needed
4. Retry failed webhooks from Stripe dashboard

For configuration issues:
1. Update webhook endpoint URL in Stripe dashboard
2. Update signing secret in Make.com
3. Test with sample webhook
4. Monitor Stripe delivery logs

VALIDATION:
â–¡ Test order placed and processed successfully
â–¡ Webhook deliveries showing successful in Stripe dashboard
â–¡ No errors in Make.com execution history
â–¡ Orders appearing in database with correct data
â–¡ Alert cleared

POST-INCIDENT:
1. Document timeline in incident log
2. Calculate downtime and affected orders
3. Determine root cause
4. Update monitoring to detect earlier
5. Schedule post-mortem within 48 hours

PREVENTION:
â†’ Add redundant webhook endpoint
â†’ Implement webhook queuing for resilience
â†’ Add pre-deployment validation checklist
â†’ Set up change management process
```

6.3.3 Incident Communication

Internal communication template:

Initial notification (within 5 minutes of detection):
```
ğŸš¨ INCIDENT: SEV-2 - Printful API Slow Response

STATUS: Investigating
IMPACT: Orders delayed 5-10 minutes, no failures yet
STARTED: 2025-11-16 10:45 UTC
LEAD: @engineer-name

Actions in progress:
- Checking Printful status page
- Reviewing error logs
- Preparing failover to Printify

Updates every 15 minutes or on status change.
```

Progress update (every 15-30 minutes):
```
ğŸ“Š INCIDENT UPDATE: SEV-2 - Printful API Slow Response

STATUS: Mitigated
IMPACT: Failover to Printify active, orders processing normally
DURATION: 23 minutes

What happened:
- Printful API response time increased from 800ms to 8000ms
- Automated failover triggered after 3 consecutive slow responses
- All new orders routing to Printify

Next steps:
- Monitoring Printful status for recovery
- Will return to normal routing when Printful stabilizes
- No customer action required

Update in 30 minutes or when resolved.
```

Resolution notification:
```
âœ… INCIDENT RESOLVED: SEV-2 - Printful API Slow Response

STATUS: Resolved
IMPACT: 47 orders delayed average 8 minutes, zero failures
DURATION: 52 minutes total
RESOLVED: 2025-11-16 11:37 UTC

What happened:
- Printful experienced elevated load causing slow API responses
- Our failover system automatically routed to Printify
- Printful recovered and we returned to normal routing

Customer impact:
- 47 orders processed during incident
- All fulfilled successfully
- Average delay: 8 minutes (within acceptable range)
- No customer notifications needed

Follow-up:
- Post-mortem scheduled for 2025-11-17 10:00am
- Will review failover threshold tuning
- Will add Printful status page monitoring

Thanks to @team for quick response!
```

External communication (if customer-facing):

Status page update:
```
November 16, 2025 11:40 UTC - Resolved

Our system experienced brief delays in order processing between 10:45 and 11:37 UTC today. All orders were processed successfully with an average delay of 8 minutes. No action is required from customers.

We've identified the cause and taken steps to prevent similar delays in the future.

If you have any questions about your order, please contact support@yourstore.com.
```

6.3.4 Post-Incident Review

Post-mortem template (blameless):

```
INCIDENT POST-MORTEM

Incident ID: INC-2025-11-16-001
Date: November 16, 2025
Duration: 52 minutes (10:45 to 11:37 UTC)
Severity: SEV-2
Responders: [Names]

SUMMARY:
Printful API response times increased tenfold causing order processing delays.
Automated failover to Printify worked as designed. 47 orders affected with
average 8 minute delay. Zero order failures.

TIMELINE:
10:45 - Alert triggered: High API response time
10:47 - Investigation started
10:50 - Printful status page confirmed elevated load
10:52 - Failover to Printify initiated
10:54 - New orders processing normally via Printify
11:15 - Printful response times returning to normal
11:25 - Gradual shift back to Printful started
11:37 - Incident declared resolved
11:40 - Status page updated

ROOT CAUSE:
Printful experienced unexpected load spike during their peak hours. Our monitoring
detected the degradation quickly, and failover worked as designed. No issues
with our systems.

WHAT WENT WELL:
âœ“ Automatic detection within 2 minutes
âœ“ Failover triggered automatically per design
âœ“ Zero order failures despite provider issues
âœ“ Communication clear and timely
âœ“ Team response time excellent

WHAT NEEDS IMPROVEMENT:
â†’ Consider pre-emptive failover based on Printful status page
â†’ Tune failover threshold (currently 3 slow requests, could be 2)
â†’ Add customer-facing ETA adjustments during provider issues
â†’ Document failover procedures for new team members

ACTION ITEMS:
[ ] Monitor Printful status page via API (Owner: @eng1, Due: Nov 20)
[ ] Adjust failover threshold in config (Owner: @eng2, Due: Nov 18)
[ ] Create customer communication templates (Owner: @support, Due: Nov 23)
[ ] Update incident response training (Owner: @lead, Due: Nov 30)

LESSONS LEARNED:
- Redundancy investment pays off immediately
- Automated failover reduces stress and human error
- Proactive monitoring prevents many customer complaints
- Clear communication during incidents builds trust
```

Production Reality Box:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ PRODUCTION REALITY: Blameless Post-Mortems Improve Systems                  â”‚
â”‚                                                                             â”‚
â”‚ Teams that focus on blame during post-mortems see repeated incidents        â”‚
â”‚ because engineers hide problems. Teams with blameless post-mortems that     â”‚
â”‚ focus on system improvements see 60-75% fewer repeat incidents. The goal    â”‚
â”‚ is learning and prevention, not punishment.                                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Validation checkpoint:
  â–¡ All SEV-1 and SEV-2 incidents have runbooks
  â–¡ Team trained on incident response procedures
  â–¡ Communication templates ready for internal and external use
  â–¡ Post-mortem process defined with action item tracking
  â–¡ Incident metrics tracked (MTTD, MTTR, recurrence rate)

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

[Content continues with Sections 6.4, 6.5, 6.6, then Parts 7 and 8, followed by Appendices A through G]

[Note: This file contains the framework. The actual implementation would continue with the remaining sections following the same detailed, production-focused pattern with specific code examples, metrics, time estimates, and Production Reality boxes.]
